<h1><a id="chapter3">Seamless deployments</a></h1>
<p>
    Reducing or eliminating downtime during a deployment becomes increasingly critical as the number and frequency of
    deployments increases. There are a number of common deployment strategies to reduce downtime or ensure our
    applications remain available throughout a deployment, resulting in seamless deployments. Given the range of high
    quality networking solutions that are available in the Kubernetes ecosystem, Kubernetes deployments are well placed
    to take advantage of these strategies.
</p>
<p>
    In this chapter we'll look at the common deployment strategies and provide examples of how these strategies can be
    realized within Octopus.
</p>
<h2>Dealing with databases during deployments</h2>
<p>
    No discussion on seamless deployments can begin without first addressing the issue of database updates.
</p>
<p>
    A fundamental aspect of most seamless deployment strategies involves running two versions of your application side
    by side, if only for a short period of time. If both versions of the application access a shared database, then any
    updates to the database schema and data must be compatible with both application versions. This is referred to as
    backward and forward compatibility.
</p>
<p>
    However, backward and forward compatibility is not trivial to implement. In the presentation Update your Database
    Schema with Zero Downtime Migrations available at <a
        href="https://www.youtube.com/watch?v=3mj6Ni7sRN4">https://www.youtube.com/watch?v=3mj6Ni7sRN4</a> (based on
    chapter 3 of the book Migrating to Microservice Databases available at <a
        href="https://developers.redhat.com/books/migrating-microservice-databases-relational-monolith-distributed-data">https://developers.redhat.com/books/migrating-microservice-databases-relational-monolith-distributed-data</a>),
    Edison Yanaga walks through the process of renaming a single column in a database. It involves six incremental
    updates to the database and application code, and all six versions to be deployed sequentially.
</p>
<p>
    Needless to say, seamless deployments involving databases require a great deal of planning, many small steps to roll
    out the changes, and tight coordination between the database and application code.
</p>
<p>
    We won't dive into the specifics of how to engineer an application with forward and backward database compatibility
    in this book. This is an application design concern, and whether or not an application supports forward and backward
    database compatibility is already determined when it is to be deployed. The deployment strategy does need to take
    into account whether an application supports running two versions side by side, but can not add this functionality
    if it isn't already built into the application.
</p>
<h2>Common deployment strategies</h2>
<p>
    There are multiple strategies to manage a cutover between an existing deployment and a new one.
</p>
<h3>Recreate</h3>
<p>
    The recreate strategy does not provide a seamless deployment, but is included here as it is the default option for
    most deployment processes. This strategy involves either removing the existing deployment and deploying the new
    version, or deploying the new version over the top of the existing deployment.
</p>
<p>
    Both options result in downtime during the period between the existing version being stopped or removed and the new
    version being started. However, because the existing and new versions are not run concurrently, database upgrades
    can be applied as needed with no backward and forward compatibility requirements.
</p>
<h3>Rolling updates</h3>
<p>
    The rolling update strategy involves incrementally updating instances of the current deployment with the new
    deployment. This strategy ensures there is always at least one instance of the current or new deployment available
    during the rollout. This requires that any shared database must maintain backward and forward compatibility.
</p>
<h3>Canary deployments</h3>
<p>
    The canary deployment strategy is similar to the rolling update strategy in that both incrementally expose more end
    users to the new deployment. The difference is that the decision to progress the rollout in a canary deployment is
    either made automatically by a system monitoring metrics and logs to ensure the new deployment is performing as
    expected, or manually by a human.
</p>
<p>
    Canary deployments also have the option to halt the rollout and revert back to the existing deployment if a problem
    is discovered.
</p>
<h3>Blue/green deployments</h3>
<p>
    The blue/green strategy involves deploying the new version (referred to as the green version) alongside the current
    version (referred to as the blue version), without exposing the green version to any traffic. Once the green version
    is deployed and verified, traffic is cutover from the blue to the green version. When the blue version is no longer
    used, it can be removed.
</p>
<p>
    Any database changes deployed by the green version must maintain backward and forward compatibility, because even if
    the green version is not serving traffic, the blue version will be exposed to the database changes.
</p>
<h3>Session draining</h3>
<p>
    The session draining strategy is used when applications maintain states tied to a particular application version.
</p>
<p>
    This strategy is similar to the blue/green strategy in that both will deploy the new version alongside the current
    version, and run both side by side. Unlike the blue/green strategy, which will cut all traffic over to the new
    deployment in a single step, the session draining strategy will direct new sessions to the new deployment, while the
    existing deployment serves traffic to existing sessions.
</p>
<p>
    After the old sessions have expired, the existing deployment can be deleted.
</p>
<p>
    Because the current and new deployments run side by side, any database changes must maintain backward and forward
    compatibility.
</p>
<p>
    Session draining was particularly important for older server side stateful web applications. This style of
    application has largely gone out of style, in large part because it made it difficult to manage with PaaS offerings
    that often treated any individual application instance as ephemeral. This book won't cover session draining in
    Kubernetes clusters.
</p>
<h3>Feature flags</h3>
<p>
    The feature flag strategy involves building functionality into a new application version, and then exposing or
    hiding the feature for select end users outside of the deployment process.
</p>
<p>
    In practice, the deployment of a new application version with flaggable features will be performed with one of the
    strategies above, so the feature flag strategy is a complement to those other strategies.
</p>
<p>
    A more limited approach to feature flags may be to enable or disable features for an entire deployment. For example,
    a new feature may be exposed to testers in non-production environments, but be disabled entirely when deployed to
    production. This broader approach is something that can be managed as part of a deployment.
</p>
<h3>Feature branch</h3>
<p>
    The feature branch strategy allows developers to deploy an application version with changes they are currently
    implementing, usually in a non-production environment, alongside the main deployment.
</p>
<p>
    It may not be necessary to maintain database backward and forward compatibility with feature branch deployments.
    Because feature branches are for testing and tend to be short-lived, it may be acceptable that each feature branch
    deployment has access to its own test database.
</p>
<h2>Seamless {{ site.platform }} deployments</h2>
<p>
    Kubernetes has out of the box support for the rolling deployment strategy for pods managed by a deployment,
    statefulset, or daemonset. Kubernetes also supports the recreate strategy for pods managed by a deployment. This
    gives us tick box support to utilize these strategies while deploying our applications.
</p>
<p>
    More advanced deployment strategies like blue/green or canary will require some planning during a deployment, or
    rely on third party networking tools to provide fine grained control over which services respond to a request.
</p>
<p>
    Kubernetes has no concept of feature branch deployments. However, with some sensible naming conventions it is
    possible to implement feature branch deployments in a relatively straightforward manner.
</p>
<h3>Rolling and recreate deployments</h3>
<p>
    Although we didn't call it out in <a href="#chapter1">chapter 1</a>, the deployment process we created is already
    configured to perform a rolling update. All Kubernetes deployment resources will be updated using either a rolling
    or recreate strategy, and the rolling update strategy was selected for us by default.
</p>
<p>
    To make either strategy meaningful we need to deploy more than one pod, so open the <b>Random Quotes - Frontend</b>
    project, select the <b>Deploy Kubernetes containers</b> step that deploys the frontend application, and then change
    the replicas count to 5. This will result in 5 pods being created hosting the frontend web application, with traffic
    distributed between them by the service.
</p>
<p>
    Deploy the <b>Random Quotes - Frontend</b> project to the <b>Development</b> environment with the new replica count
    to populate the cluster with the new pods.
</p>
<p>
    Now redeploy the <b>Random Quotes - Frontend</b> project to the <b>Development</b> environment. When Octopus
    presents the package versions, expand the packages section, click the <b>SELECT VERSION</b> button next to the
    <b>octopussamples/randomquotesjava</b> package, tick the <b>Pre-release packages</b> option, and select the
    <b>0.1.9-purpleheader</b> version. This version of the image replaces the background color of the header displayed
    by the web application. Click the <b>OK</b> button to select the new version.
</p>
<p class="note">
    Although Docker tags have no concept of a pre-release, Octopus treats tags like <b>0.1.9-purpleheader</b> as a
    prerelease version, with the identifier <b>purpleheader</b> indicating this is a pre-release.
</p>
<p>
    The log files generated during the deployment show entries like <b>Waiting for deployment "webapp" rollout to
        finish: 4 out of 5 new replicas have been updated...</b>, followed by entries like <b>Waiting for deployment
        "webapp" rollout to finish: 2 old replicas are pending termination...</b>. This indicates that Kubernetes is
    deploying pods with the new image before deleting pods with the old image.
</p>
<p>
    The diagram below shows how a rolling update strategy progresses, with old pods progressively deleted and replaced
    with new pods:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/rolling-deployment.png" /></div>
<p>
    Go back to the deployment process for the <b>Random Quotes - Frontend</b> project, select the <b>Deploy Kubernetes
        containers</b> step deploying the frontend application, expand the <b>Deployment Strategy</b> section, and
    select the <b>Recreate deployments</b> option. Save the changes and redeploy the project to the <b>Development</b>
    environment.
</p>
<p>
    Notice now the log files now only have messages like <b>Waiting for deployment "webapp" rollout to finish: 0 out of
        5 new replicas have been updated...</b>, with the replica count increasing until all pods have been deployed.
    This indicates that the old pods were removed before any new pods were deployed.
</p>
<p>
    The diagram below shows how a recreate update strategy progresses, where all old pods are first deleted, and new
    pods are then deployed:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/recreate-deployment.png" /></div>
<h3>Blue/green deployments</h3>
<p>
    Unlike the rolling and recreate strategies, blue/green deployments are not natively supported by Kubernetes.
    However, Octopus does provide the ability to perform blue/green deployments by orchestrating the various Kubernetes
    resources for you.
</p>
<p>
    Open the <b>Random Quotes - Frontend</b> project, select the <b>Deploy Kubernetes containers</b> step deploying the
    frontend application, expand the <b>Deployment Strategy</b> section, and select the <b>Blue/Green deployments</b>
    option. Save the changes and deploy the project to the <b>Development</b> environment.
</p>
<p>
    Notice that the logs contains entries like <b>deployment.apps/webapp-deployments-101 created</b>. Of particular note
    is that the name of the deployment is made up of the name that was entered into the step, which was <b>webapp</b>,
    and a suffix like <b>-deployments-101</b>.
</p>
<p>
    This is a crucial aspect of blue/green deployments. To have the blue and green stacks coexist side by side in the
    cluster, resources like the Kubernetes deployment must have unique names. Octopus ensures this by appending the
    deployment ID to the end of the deployment name. This ensures each deployment orchestrated by Octopus generates a
    unique Kubernetes deployment resource.
</p>
<p>
    Note though that not all resources created by this step have a unique name. There is only one service created as
    part of a blue/green deployment, so it retains the name of <b>webapp</b>.
</p>
<p>
    After the first blue/green deployment, our cluster looks like this. We have a shared service pointing to a uniquely
    named deployment. The deployment was successful, so the new deployment becomes the blue stack:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen1.png" /></div>
<p>
    Now lets deploy the project to the <b>Development</b> environment again. Notice now the logs contain entries like
    <b>deployment.apps/webapp-deployments-102 created</b>. This is a new deployment, so the suffix appended to the
    deployment resource has been updated.
</p>
<p>
    As the new deployment resource is spun up in the cluster, the state of the cluster looks like this. The new
    deployment, considered to be the green stack, is created but not yet receiving traffic:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen2.png" /></div>
<p>
    Once the new deployment resource has passed all of its health checks and is considered fully available, the service
    is updated to direct traffic to it:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen3.png" /></div>
<p>
    The resources in the blue stack are now deleted:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen4.png" /></div>
<p>
    With the deployment process now done, the resources from the green stack are considered to be the blue stack:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen5.png" /></div>
<p>
    So, in summary, these are the steps Octopus will orchestrate for a blue/green deployment:
</p>
<ol>
    <li>Create a Kubernetes deployment resource with a unique name in the green stack.</li>
    <li>Wait for the new deployment to pass its health checks and be available.</li>
    <li>Update a shared service to point to the new deployment resource.</li>
    <li>Delete the old deployment resources in the blue stack.</li>
    <li>At the end of the deployment, the green stack is considered to be the blue stack.</li>
</ol>
<h3>The opinions embedded in the Deploy Kubernetes containers step</h3>
<p>
    While performing a blue/green deployment we saw that Octopus will ensure certain resources have unique names by
    appending the ID of the Octopus deployment to the end. In the case of a deployment resource, this is required to
    allow the green and blue stacks to coexist while traffic is cut over from the blue stack to the green stack.
</p>
<p>
    Any secrets, configmaps, or custom resources defined in the <b>Deploy Kubernetes containers</b> step also have their
    names altered to ensure they are unique, regardless of the deployment strategy.
</p>
<p class="note">
    A custom resource is any Kubernetes resource that can be defined in YAML. The ability to define a custom resource is
    enabled by clicking the <b>CONFIGURE FEATURES</b> button and selecting the <b>Custom Resource</b> option.
</p>
<p>
    Providing unique names for these resources is one of the opinions baked into the <b>Deploy Kubernetes containers</b>
    step. The step assumes that these resources exist to support the main deployment resource, and that they are tightly
    bound to the lifecycle of any pods managed by the deployment.
</p>
<p>
    For example, if a configmap is defined as part of the <b>Deploy Kubernetes containers</b> step, it is assumed that
    the data in the configmap is required for the pods managed by the deployment to run correctly. This might be because
    the configmap defines values that are exposed as pod environment variables, or mounted as files.
</p>
<p>
    By ensuring the configmap has a unique name for each deployment, we can be sure that during a rolling or blue/green
    update, the old pods have access to the data in the old configmaps, and the new pods have access to the data in the
    new configmaps. If both the old and the new pods had shared a single configmap, it would be possible that any new
    data in the shared configmap would break the old pods in unexpected ways.
</p>
<p>
    Resources created by all other Kubernetes steps in Octopus do not alter the resource's names. So if, for example,
    you need to create a configmap that is shared between all pods at all times, you can use the <b>Deploy Kubernetes
        config map resource</b> step, which will create a configmap with the name supplied in the step with no
    modification.
</p>
<h3>Preparing for a canary deployment</h3>
<p>
    Octopus managed the blue/green deployment for us with a hard cutover from the blue to the green stack once the green
    stack was ready. Canary deployments are different in that only a subset of traffic is directed to the new green
    stack. The green stack traffic is incrementally increased until eventually all traffic has migrated from the blue
    stack to the green stack.
</p>
<p>
    The networking features native to Kubernetes do not directly support this kind of graduated traffic increase. It
    would technically be possible to direct more traffic to the green stack through a service that selected pods from
    both the blue and green stacks, decreasing the blue stack pod count, and increasing the green stack pod count. But
    the traffic percentage changes are limited by the number of pods, making it a rather coarse solution.
</p>
<p>
    Fortunately Kubernetes has a rich ecosystem of third party networking tools that specifically support canary
    deployments. One popular networking solution is the NGINX ingress controller.
</p>
<p class="note">
    Kubernetes services coordinate layer 4 network traffic (i.e. TCP and UDP traffic) amongst pods. Ingress controllers
    provide an additional layer to direct layer 7 traffic (i.e. HTTP traffic) to services. Kubernetes provides a
    standard ingress resource, but does not supply an ingress controller to implement the ingress rules. Third party
    ingress controllers, like NGINX, are installed into a Kubernetes cluster to implement ingress rules.
</p>
<p>
    The easiest way to install tools like NGINX in a Kubernetes cluster is with Helm. Helm is like an operating system
    package manager, such as Chocolatey for Windows or Advanced Package Tool (APT) for Ubuntu, with the obvious difference being Helm is used to
    deploy applications into a Kubernetes cluster.
</p>
<p>
    To consume Helm packages, called charts, we need to add a Helm external feed. Open <b>Library -> External Feeds</b>,
    and click the <b>ADD FEED</b> button. Select <b>Helm Feed</b> as the feed type, and give the feed the name of
    <b>Helm</b>. The default URL of https://charts.helm.sh/stable is fine, so click the <b>SAVE</b> button.
</p>
<p>
    We'll then deploy the NGINX ingress controller through a new runbook in the <b>Kubernetes DevOps</b> project called
    <b>Deploy NGINX Ingress Controller</b>. Add the <b>Upgrade a Helm Chart</b> step to the runbook.
</p>
<p>
    This step will run on behalf of targets in the <b>Kubernetes Admin</b> role. The step will also need access to the
    <b>helm</b> command line tool, which we can provide via the <b>octopusdeploy/worker-tools:windows.ltsc2019</b> or
    <b>octopusdeploy/worker-tools:ubuntu.18.04</b> container images.
</p>
<p>
    Select the <b>Helm</b> feed in the chart section, and enter <b>nginx-ingress</b> as the package id. Finally enter
    <b>nginx-ingress</b> as the Kubernetes release name.
</p>
<p>
    All other default values in the step are fine for us, so save the step, and run the runbook in the <b>Admin</b>
    environment.
</p>
<p>
    The benefit of using Helm to deploy third party applications like NGINX is that a Helm chart bundles all the various
    resources needed for that application to function. One of the resources that we are interested in is the load
    balancer service created to direct public network traffic into the ingress controller. This load balancer service is
    much the same as we created in <a href="#chapter1">chapter 1</a>, except that the traffic is now sent to an ingress
    controller that can be configured with fine grained rules to route HTTP traffic amongst many internal services.
</p>
<p>
    As before, we need a way to get the public IP address of this load balancer service. Luckily we already have the
    <b>Get Service IP</b> runbook from <a href="#chapter1">chapter 1</a>. Clone this runbook into a new runbook called
    <b>Get Admin Service IP</b>. Configure the runbook step to run against targets with the <b>Kubernetes Admin</b>
    role, which will result in this runbook querying resources in the <b>default</b> namespace. Then run the runbook in
    the <b>Admin</b> environment, which displays the IP address of the NGINX load balancer service. The
    <b>EXTERNAL-IP</b> assigned to the <b>nginx-ingress-controller</b> service provides the value we need:
</p>
<div><img alt="Octopus environments" src="images/octopus/nginx-loadbalancer.png" /></div>
<p>
    We now have an ingress controller to split our canary traffic between an existing and a new deployment and its
    service. The next step is to deploy the frontend web application to support canary deployments.
</p>
<h3>Canary deployments</h3>
<p>
    While the blue/green deployment strategy was easy to implement thanks to the checkbox exposed by the <b>Deploy Kubernetes containers</b> step, it has the downside that the entire process is self contained and not
    customizable. The process the blue/green deployment strategy implements is very similar to what we want to achieve in a canary deployment though. So
    we'll go ahead and recreate the blue/green deployment strategy as individual steps in a deployment process.
</p>
<p>
    If you recall from the previous section, a blue/green deployment performs the following steps:
</p>
<ol>
    <li>Create a Kubernetes deployment resource with a unique name in the green stack.</li>
    <li>Wait for the new deployment to pass its health checks and be available.</li>
    <li>Update a shared service to point to the new deployment resource.</li>
    <li>Delete the old deployment resources in the blue stack.</li>
    <li>At the end of the deployment, the green stack is considered to be the blue stack.</li>
</ol>
<p>
    Performing a canary deployment is a little more complicated, but shares many of those steps. Our canary deployment
    will follow this process:
</p>
<ol>
    <li>Create a Kubernetes deployment resource with a unique name in the canary, or green, stack.</li>
    <li>Wait for the new deployment to pass its health checks and be available.</li>
    <li>Create a new, unique service to direct traffic to the pods in the green stack.</li>
    <li>Create a temporary ingress to direct traffic to the green stack.</li>
    <li>Incrementally increase the traffic directed to the green stack until it reaches 100%.</li>
    <li>Update the main ingress to point all traffic to the green stack.</li>
    <li>Delete the temporary ingress.</li>
    <li>Delete the old deployment resources in the blue stack.</li>
    <li>At the end of the deployment, the green stack is considered to be the blue stack.</li>
</ol>
<p>
    A new consideration to take into account is how we will access our environments using ingress rules instead of a
    load balancer service.
</p>
<p>
    Previously we exposed each environment through its own unique load balancer service. This meant the load balancer IP
    determined which environment we accessed.
</p>
<p>
    To implement canary deployments though, we now use a shared load balancer service created by the ingress controller.
    This means we need a new way to distinguish between environments, as all environments share the same public IP
    address or hostname of the single ingress controller load balancer.
</p>
<p>
    To accommodate this, we'll expose each environment under a top level path in our URLs. So to access the development
    environment, we'll open the URL http://[ingress controller load balancer]/deployment. To access the test
    environment, we'll open the URL http://[ingress controller load balancer]/test. And to access the production
    environment, we'll open the URL http://[ingress controller load balancer]/production.
</p>
<p class="note">
    This top level path in this URL is called the <i>context path</i>.
</p>
<p>
    To see how this works in practice, let's create a new project called <b>Random Quotes - Frontend - Canary</b>.
</p>
<p>
    We'll start by defining a variable called <b>ContextPath</b>, which will resolve to a unique path for each
    environment. Open the <b>Variables</b> link and add a new variable called <b>ContextPath</b> with the value of
    <b>/#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    Now go to the process editor and add a new <b>Deploy Kubernetes containers</b> step called <b>Deploy Web App</b>.
</p>
<p>
    Import the following YAML in the <b>Deployment</b> section:
</p>
<pre class="wrapPre">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        octopusexport: OctopusExport
    spec:
      containers:
        - name: webapp
          image: index.docker.io/octopussamples/randomquotesjava
          ports:
            - name: web
              containerPort: 5555
          env:
            - name: SERVER_SERVLET_CONTEXT_PATH
              value: '#{ContextPath}'
            - name: SPRING_CONFIG_NAME
              value: postgres-application
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_URL
              value: 'jdbc:postgresql://postgres.backend-#{Octopus.Environment.Name | ToLower}:5432/postgres'
            - name: POSTGRES_PASS
              value: '#{DatabasePassword}'
          readinessProbe:
            failureThreshold: 6
            periodSeconds: 20
            tcpSocket:
              host: ''
              port: 5555
</pre>
<p>
    Notice the deployment name is set to <b>webapp-canary-#{Octopus.Deployment.Id | ToLower}</b>. Generating a unique
    name with each deployment replicates the functionality that Octopus provided automatically when performing a
    blue/green deployment.
</p>
<p>
    We also need to configure the Spring web application to respond to requests while being hosted under a new
    subdirectory. Spring (and most Java applications) refer to the base URL that they are deployed under as the context
    path. Spring reads the <b>SERVER_SERVLET_CONTEXT_PATH</b> environment variable to configure the context path, which is now set to the <b>ContextPath</b> variable we created earlier.
</p>
<p>
    Otherwise the configuration of this deployment is the same as we configured previously.
</p>
<p>
    Under the <b>Service</b> section import the following YAML:
</p>
<pre class="wrapPre">
apiVersion: v1
kind: Service
metadata:
  name: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
spec:
  type: ClusterIP
  ports:
    - name: web
      port: 80
      targetPort: 5555
      protocol: TCP
  selector:
    octopusexport: OctopusExport
</pre>
<p>
    Again we have given this resource a unique name, ensuring each time we deploy this project with Octopus a new service
    will be created.
</p>
<p>
    By creating a service for each deployment, we will end up with two services running along side the two deployments.
    This is different from the blue/green deployment where there was one service that either pointed to the green or
    blue stacks, and updating the service resulted in the traffic being cut over to the new green stack. Having two
    services is crucial for canary deployments though, as the NGINX ingress controller splits traffic between two
    services.
</p>
<p>
    The next steps recreate the smoke, integration, and end-to-end tests that we introduced in <a
        href="#chapter2">chapter 2</a>. The steps are the same when performing a canary deployment, with the only change
    required being the base URL that each of the tests contacts, which has changed to
    <b>http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name |
        ToLower}#{ContextPath}</b> due to the unique name now assigned to the service with each deployment, and the new
    context path.
</p>
<p>
    The smoke test script is modified with the new URLs:
</p>
<pre class="wrapPre">
sleep 30

# Contact the HTTP API
curl -sSf http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath}/api/quote > /dev/null

# Fail if curl failed
if [[ $? != 0 ]]; then
	return 1
fi;

# Contact the web frontend
curl -sSf http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath} > /dev/null
</pre>
<p>
    The integration and end-to-end tests have their URLs modified via the variables used by the structured variable
    replacement feature. The new variables will be set to:
</p>
<ul>
    <li><b>baseUrl</b> = <b>http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name
            | ToLower}#{ContextPath}</b></li>
    <li><b>item:0:request:url:host:0</b> = <b>webapp-canary-#{Octopus.Deployment.Id | ToLower}</b></li>
    <li><b>item:0:request:url:host:1</b> = <b>frontend-#{Octopus.Environment.Name | ToLower}</b></li>
    <li><b>item:0:request:url:raw</b> = <b>http://webapp-canary-#{Octopus.Deployment.Id |
            ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath}</b></li>
</ul>
<p>
    We also need to update the path array in the Postman integration test JSON, which is set to <b>"path":
        ["api","quote"]</b>. This array now needs to include the context path, meaning the array needs to look like
    <b>["#{Octopus.Environment.Name | ToLower}","api","quote"]</b>.
</p>
<p>
    We can achieve this by using structured configuration variables to replace the entire array rather than replacing
    individual items inside it. Create a new variable called <b>item:0:request:url:path</b> and set the value to
    <b>["#{Octopus.Environment.Name | ToLower}", "api", "quote"]</b>. This builds up a JSON array, and replaces the
    existing array with the new values.
</p>
<p>
    The result of these new variables means the Postman JSON file will look like this after processing:
</p>
<pre class="wrapPre">
{
    "info": {
        "_postman_id": "2c6a5ce5-e2af-4f23-ab6c-172d0dfa3797",
        "name": "RandomQuotes",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Quotes API",
            "event": [
                {
                    "listen": "test",
                    "script": {
                        "exec": [
                            "pm.test(\"response must be valid and have a body\", function () {\r",
                            "     pm.response.to.be.ok;\r",
                            "     pm.response.to.be.withBody;\r",
                            "     pm.response.to.be.json;\r",
                            "\r",
                            "     pm.expect(pm.response.json().quote != \"\").to.be.true;\r",
                            "});"
                        ],
                        "type": "text/javascript"
                    }
                }
            ],
            "request": {
                "method": "GET",
                "header": [],
                "url": {
                    "raw": "http://webapp-canary-deployments-511.frontend-development/development/api/quote",
                    "protocol": "http",
                    "host": [
                        "webapp-canary-deployments-511",
                        "frontend-development"
                    ],
                    "path": [
                        "development",
                        "api",
                        "quote"
                    ]
                }
            },
            "response": []
        }
    ]
}
</pre>
<p class="note">
    Replacing two values and regenerating an array is arguably pushing the limits of the transforms you would expect to
    perform as part of a deployment. We've implemented these changes here to demonstrate the flexibility of structured
    variable replacements. However, the more changes you make to a template, the more risk you run that your injected
    values diverge from the underlying JSON file. Postman has the ability to define its own variables which can be
    passed in as command line arguments to <b>newman</b>, which is a more robust solution for creating environment
    agnostic tests.
</p>
<p>
    While we are modifying variables, add a new variable called <b>Hostname</b>. The value of this variable will be set
    to the hostname of the ingress controller load balancer.
</p>
<p>
    Different cloud providers expose load balancer hostnames differently. Google cloud uses the format <b>[reversed ip
        address].bc.googleusercontent.com</b>, where a load balancer ip of <b>34.123.125.215</b> has a hostname of
    <b>215.125.123.34.bc.googleusercontent.com</b>. Azure allows the annotation
    <b>service.beta.kubernetes.io/azure-dns-label-name</b> to be defined on a load balancer service, which generates a
    hostname like <b>[dns label].[region].cloudapp.azure.com</b> (e.g. mydnslabel.centralus.cloudapp.azure.com). AWS
    Elastic Kubernetes Service (EKS) load balancers expose a hostname directly to Kubernetes, so it will appear as the
    external IP when the <b>kubectl get services</b> command is run.
</p>
<p>
    Because we are using GKE in this book, the value of the <b>Hostname</b> variable will be set to <b>[reversed ip
        address].bc.googleusercontent.com</b>.
</p>
<p>
    Once the tests pass we will direct a subset of incoming traffic to the canary stack (or green stack in keeping with
    the terminology we have already used in blue/green deployments) via an ingress. This is done with a <b>Deploy
        Kubernetes ingress resource</b> step. Like all Kubernetes steps, the <b>Deploy Kubernetes ingress resource</b>
    step allows YAML to be directly imported. The YAML below defines the ingress required to direct 50% of traffic to
    the green stack:
</p>
<pre class="wrapPre">
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
    name: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
    annotations:
    nginx.ingress.kubernetes.io/canary-weight: '50'
    nginx.ingress.kubernetes.io/canary: 'true'
spec:
    ingressClassName: nginx
    rules:
    - host: '#{Hostname}'
        http:
        paths:
            - path: '/#{ContextPath}'
            backend:
                servicePort: web
                serviceName: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
</pre>
<p>
    The annotations assigned to the ingress configure settings are recognized by the NGINX ingress controller. The
    <b>nginx.ingress.kubernetes.io/canary-weight</b> annotation defines how much traffic to send to the service for any
    incoming traffic matching the ingress rules. The <b>nginx.ingress.kubernetes.io/canary</b> annotation is set to
    <b>true</b> to indicate that this ingress rule is used to split traffic with a second main ingress rule (which we
    will create in later steps).
</p>
<p>
    The ingress rules define that any HTTP traffic to the load balancer hostname will be directed to the green stack
    service.
</p>
<p class="note">
    Although a hostname is optional on ingress resources, NGINX requires a hostname in order to split canary traffic.
</p>
<p>
    At this point in the deployment we have 50% of traffic being directed to our green stack. The central difference
    between a canary and blue/green deployment is that a canary deployment gives us control over when to direct more
    traffic to the green stack.
</p>
<p>
    So at this point we will pause the deployment and wait for confirmation that we can direct more traffic to the green
    stack. In Octopus this is achieved with a <b>Manual Intervention Required</b> step. Add a new <b>Manual Intervention
        Required</b> step and set the instructions to <b>Do you wish to increase the canary traffic to 100%?</b> When
    this step is reached in a deployment, Octopus will pause and wait for an operator to approve the decision to direct
    the remaining traffic to the green stack.
</p>
<p>
    The next step then is to direct all traffic to the green stack. We do this by creating a regular (i.e. unannotated)
    ingress resource with a second <b>Deploy Kubernetes ingress resource</b> step. The YAML for this step is shown
    below:
</p>
<pre class="wrapPre">
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
    name: webapp-canary
spec:
    ingressClassName: nginx
    rules:
    - host: '#{Hostname}'
        http:
        paths:
            - path: '/#{ContextPath}'
            backend:
                servicePort: web
                serviceName: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
</pre>
<p>
    We now have two ingress rules, both directing traffic to the green stack. The temporary canary ingress is no longer
    required, so this is deleted with a <b>Run a kubectl CLI Script</b> step that has the following script:
</p>
<pre class="wrapPre">
kubectl delete ingress webapp-canary-#{Octopus.Deployment.Id | ToLower}
</pre>
<p>
    No traffic is being directed to the blue stack any more, so we can clean it up with a <b>Run a kubectl CLI
        Script</b> step that has the following script:
</p>
<pre class="wrapPre">
kubectl delete service -l Octopus.Project.Id=#{Octopus.Project.Id | ToLower},Octopus.Environment.Id=#{Octopus.Environment.Id | ToLower},Octopus.Deployment.Tenant.Id=#{unless Octopus.Deployment.Tenant.Id}untenanted#{/unless}#{if Octopus.Deployment.Tenant.Id}#{Octopus.Deployment.Tenant.Id | ToLower}#{/if},Octopus.Deployment.Id!=#{Octopus.Deployment.Id | ToLower}

kubectl delete deployment -l Octopus.Project.Id=#{Octopus.Project.Id | ToLower},Octopus.Environment.Id=#{Octopus.Environment.Id | ToLower},Octopus.Deployment.Tenant.Id=#{unless Octopus.Deployment.Tenant.Id}untenanted#{/unless}#{if Octopus.Deployment.Tenant.Id}#{Octopus.Deployment.Tenant.Id | ToLower}#{/if},Octopus.Deployment.Id!=#{Octopus.Deployment.Id | ToLower}
</pre>
<p>
    Those commands can look a little daunting, so let's break them down. Each command is deleting old resources with a
    command like <b>kubectl delete [resource]</b>. The resources to be deleted are matched by their labels with the
    <b>-l</b> flag. We match resources based on labels that are automatically added by Octopus. The overall label
    matching expression is broken down by commas:
</p>
<ul>
    <li><b>Octopus.Project.Id=#{Octopus.Project.Id | ToLower}</b> matches resources created by the same project.</li>
    <li><b>Octopus.Environment.Id=#{Octopus.Environment.Id | ToLower}</b> matches resources created in the same
        environment.</li>
    <li><b>Octopus.Deployment.Tenant.Id=#{unless Octopus.Deployment.Tenant.Id}untenanted#{/unless}#{if
            Octopus.Deployment.Tenant.Id}#{Octopus.Deployment.Tenant.Id | ToLower}#{/if}</b> matches resources created
        for the same tenant.</li>
    <li><b>Octopus.Deployment.Id!=#{Octopus.Deployment.Id | ToLower}</b> matches resources that were not created
        by the current deployment, which means it only matches resources created by previous deployments.</li>
</ul>
<p class="note">
    Octopus tenants provide an additional dimension along which deployments can be managed. The canonical use case is
    where an application is deployed independently for multiple customers. For example, Acme Corporation may receive
    each minor release of an application, while Globex Corporation may only receive major releases. Defining these
    companies as tenants allows their deployments to be managed separately, while sharing a single deployment project.
</p>
<p>
    The end result of these <b>kubectl</b> commands is to delete any resources created by a previous deployment by this
    project in the same environment for the same tenant. Or, in other words, deleting the blue stack.
</p>
<p>
    If any of the tests failed or we decided not to proceed with the complete rollout, we will clean up the green stack.
    This is done with another <b>Run a kubectl CLI Script</b> step. However, this step will only run when the deployment
    process fails. This is done by setting the <b>Run Condition</b> to <b>Failure: only run when the previous steps
        fail</b>.
</p>
<p>
    The script is then set to:
</p>
<pre class="wrapPre">
kubectl delete ingress -l Octopus.Deployment.Id=#{Octopus.Deployment.Id | ToLower}

kubectl delete service -l Octopus.Deployment.Id=#{Octopus.Deployment.Id | ToLower}

kubectl delete deployment -l Octopus.Deployment.Id=#{Octopus.Deployment.Id | ToLower}
</pre>
<p>
    The label matches here use a simplified expression that match on the deployment ID
    <b>Octopus.Deployment.Id=#{Octopus.Deployment.Id | ToLower}</b>. This means resources created as part of the current
    deployment are cleaned up.
</p>
<h3>Canary deployments in pictures</h3>
<p>
    Creating our canary deployment was not a trivial undertaking, and it can be difficult to understand exactly what all
    the steps above did, so lets run through the process with diagrams demonstrating the high level workflow.
</p>
<p>
    We start with the blue stack receiving 100% of traffic:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/canary1.png" /></div>
<p>
    The green stack is created and tested:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/canary2.png" /></div>
<p>
    A canary ingress is created directing 50% of traffic to the green stack:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/canary3.png" /></div>
<p>
    The decision is made to direct 100% of traffic to the green stack, so the main ingress is reconfigured and the
    temporary service and ingress are deleted:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/canary4.png" /></div>
<p>
    The green stack becomes the blue stack:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/canary5.png" /></div>
<h3>Feature branch deployments</h3>
<p>
    Conceptually, deploying feature branches is easy to understand, as all it involves is deploying a new container built from a
    feature branch side by side with the container built from the mainline branch. This does require some reconfiguration of our deployment process though. In this section we will extend the
    canary deployment process described above to allow multiple feature branches to be deployed side by side.
</p>
<p>
    In practice feature branches would likely use the recreate strategy in test environments, because internal testers
    are not likely to be impacted all that much by momentary downtime of a feature branch release. However, given canary
    deployments are one of the most complex deployment strategies you can implement, if we can get feature branches
    working with canary deployments, we can get them working anywhere.
</p>
<p>
    This raises two questions: How do we identify feature branch images, and how do we exposed feature branch
    deployments alongside the main deployment?
</p>
<p>
    The most common way to identify feature branch images is via the prerelease identifier on the image tag. This means
    the tag <b>0.1.9-purpleheader</b> we used earlier represents a feature branch called <b>purpleheader</b>. This works
    nicely because feature branches are applications under development, and are therefor prereleases.
</p>
<p>
    To access a feature branch deployment, we'll expose them under a subdirectory in the URL, such as http://[reversed
    ip address].bc.googleusercontent.com/development-purpleheader. This leaves the main deployment available at a URL
    like http://[reversed ip address].bc.googleusercontent.com/development.
</p>
<p>
    To extract the prerelease component from the image being deployed, we create a new variable called
    <b>PackagePreRelease</b> with the value <b>#{Octopus.Action[Deploy Web App].Package[webapp].PackageVersion |
        VersionPreReleasePrefix}</b>. This takes the package version from the step called <b>Deploy Web App</b> and runs
    it through the filter <b>VersionPreReleasePrefix</b>. This filter extracts the string <b>purpleheader</b> from a
    version like <b>0.1.9-purpleheader</b>.
</p>
<p class="note">
    Octopus contributes a lot of variables that can be consumed during a deployment process. These variables are not
    typically documented, but by setting the <b>OctopusPrintVariables</b> variable to true, all of the available
    variables are printed to the deployment logs. A second variable called <b>OctopusPrintEvaluatedVariables</b> can be
    set to <b>True</b> to print the final evaluated value of each variable (i.e. the values of variables once any nested
    variable expressions have been resolved) to the logs. It is then possible to scan the logs for any interesting
    variables that you may want to consume in a deployment.
</p>
<p>
    The feature branch name (i.e. the package prerelease) will be used in two different ways in our deployment. First,
    the feature branch name with a dash prepended will be included in the names of Kubernetes resources. However,
    because the mainline containers have no prerelease component, and therefor have no feature branch name, we only want
    to include the feature branch name in the resource names when there is a feature branch name to include.
</p>
<p>
    To accommodate this we create a variable called <b>ServiceSuffix</b> set to the value of <b>#{if
        PackagePreRelease}-#{PackagePreRelease}#{/if}</b>. This variable syntax says that if the
    <b>PackagePreRelease</b> has any value, <b>ServiceSuffix</b> is set to a dash and then the value of
    <b>PackagePreRelease</b>. If <b>PackagePreRelease</b> is an empty string, <b>ServiceSuffix</b> is also an empty
    string.
</p>
<p>
    We also want to expose the feature branch name as a URL path component. Update the variable called <b>ContextPath</b>
    and with the value of <b>/#{Octopus.Environment.Name | ToLower}#{if
        PackagePreRelease}-#{PackagePreRelease}#{/if}</b>. Here we expose the different environments though a shared
    hostname by ensuring they have unique paths based on the environment name. Then, like the <b>ServiceSuffix</b>
    variable, <b>ContextPath</b> will append empty string if <b>PackagePreRelease</b> is an empty string. Otherwise
    <b>ContextPath</b> is appended with a dash and then the value of <b>PackagePreRelease</b>.
</p>
<p>
    The smoke test needs to be updated with the new URLs for the feature branch tests. Set the script to:
</p>
<pre class="wrapPre">
# Contact the HTTP API
curl -sSf http://webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath}#{/if}/api/quote > /dev/null

# Fail if curl failed
if [[ $? != 0 ]]; then
	return 1
fi;

# Contact the web frontend
curl -sSf http://webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath}#{/if} > /dev/null
</pre>
<p>
    Our integration and end-to-end tests also need to be updated to reflect the new service hostnames and path.
</p>
<p>
    The value of the host array used by Postman is set to <b>"host": ["webapp","frontend-development"]</b>. The first
    item in this array needs to be changed to <b>"host": ["webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}","frontend-development"]</b>. So we update the variable called <b>item:0:request:url:host:0</b> to <b>webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}</b>.
</p>
<p>
    The value of the raw url is set to <b>"raw": "http://webapp.frontend-development/api/quote"</b>. This needs to be
    changed to <b>http://webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath}#{/if}</b>. So we update the <b>item:0:request:url:raw</b> variable to <b>http://webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}#{ContextPath}#{/if}</b>.
</p>
<p>
    The path array which is set to <b>"path": ["api","quote"]</b>. This array will need to be <b>["/#{Octopus.Environment.Name | ToLower}#{if PackagePreRelease}-#{PackagePreRelease}#{/if}","api","quote"]</b>. So update the variable <b>item:0:request:url:path</b> and set the value to <b>["#{Octopus.Environment.Name | ToLower}#{if PackagePreRelease}-#{PackagePreRelease}#{/if}", "api", "quote"]</b>.
</p>
<p>
    The end result of these variable changes is that the Postman JSON file looks like this at deployment time:
</p>
<pre class="wrapPre">
{
    "info": {
        "_postman_id": "2c6a5ce5-e2af-4f23-ab6c-172d0dfa3797",
        "name": "RandomQuotes",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Quotes API",
            "event": [
                {
                    "listen": "test",
                    "script": {
                        "exec": [
                            "pm.test(\"response must be valid and have a body\", function () {\r",
                            "     pm.response.to.be.ok;\r",
                            "     pm.response.to.be.withBody;\r",
                            "     pm.response.to.be.json;\r",
                            "\r",
                            "     pm.expect(pm.response.json().quote != \"\").to.be.true;\r",
                            "});"
                        ],
                        "type": "text/javascript"
                    }
                }
            ],
            "request": {
                "method": "GET",
                "header": [],
                "url": {
                    "raw": "http://webapp-canary-purpleheader-deployments-511.frontend-development/development-purpleheader/api/quote",
                    "protocol": "http",
                    "host": [
                        "webapp-canary-purpleheader-deployments-511",
                        "frontend-development"
                    ],
                    "path": [
                        "development-purpleheader",
                        "api",
                        "quote"
                    ]
                }
            },
            "response": []
        }
    ]
}
</pre>
<p>
    The base URL used by our Cypress test needs to be updated, so set the value of the <b>baseUrl</b> variable to
    <b>http://webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name |
        ToLower}#{ContextPath}#{/if}</b>.
</p>
<p>
    We need a new label on our Kubernetes resources to identify the feature branch they belong to. In the <b>Deploy
        Kubernetes containers</b> step, expand the <b>Deployment</b> section and add a label called <b>FeatureBranch</b>
    with the value of <b>#{PackagePreRelease}</b>. This will add that label to the deployment and service resource
    created by the step.
</p>
<p>
    The name of the deployment resource will be updated to <b>webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id |
        ToLower}</b>, and the name of the service will be <b>webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id |
        ToLower}</b>.
</p>
<p>
    Or, if you prefer, copy and paste the deployment YAML below. Labels defined on the deployment resource will be read
    by the <b>Deploy Kubernetes containers</b> step and applied to all resources created by the step:
</p>
<pre class="wrapPre"> 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: 'webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}'
  labels:
    FeatureBranch: '#{PackagePreRelease}'
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        FeatureBranch: '#{PackagePreRelease}'
        octopusexport: OctopusExport
    spec:
      containers:
        - name: webapp
          image: index.docker.io/octopussamples/randomquotesjava
          imagePullPolicy: Always
          ports:
            - name: web
              containerPort: 5555
          env:
            - name: SERVER_SERVLET_CONTEXT_PATH
              value: '#{ContextPath}'
            - name: SPRING_CONFIG_NAME
              value: postgres-application
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_URL
              value: 'jdbc:postgresql://postgres.backend-development:5432/postgres'
            - name: POSTGRES_PASS
              value: '#{DatabasePassword}'
          readinessProbe:
            failureThreshold: 6
            periodSeconds: 20
            tcpSocket:
              host: ''
              port: 5555
</pre>
<p>
    Here is the YAML for the updated service:
</p>
<pre class="wrapPre">
apiVersion: v1
kind: Service
metadata:
  name: 'webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}'
spec:
  type: LoadBalancer
  ports:
    - name: web
      port: 80
      targetPort: 5555
      protocol: TCP
  selector:
    octopusexport: OctopusExport
</pre>
<p>
    When cleaning up old resources, we only want to delete those resources that were previously created for the feature
    branch we are deploying. To delete the temporary ingress, the script changes to this:
</p>
<pre class="wrapPre">
kubectl delete ingress webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}
</pre>
<p>
    When deleting the blue stack resources, the script changes to this:
</p>
<pre class="wrapPre">
kubectl delete ingress -l Octopus.Project.Id=#{Octopus.Project.Id | ToLower},Octopus.Environment.Id=#{Octopus.Environment.Id | ToLower},Octopus.Deployment.Tenant.Id=#{unless Octopus.Deployment.Tenant.Id}untenanted#{/unless}#{if Octopus.Deployment.Tenant.Id}#{Octopus.Deployment.Tenant.Id | ToLower}#{/if},Octopus.Deployment.Id!=#{Octopus.Deployment.Id | ToLower},FeatureBranch=#{PackagePreRelease}

kubectl delete service -l Octopus.Project.Id=#{Octopus.Project.Id | ToLower},Octopus.Environment.Id=#{Octopus.Environment.Id | ToLower},Octopus.Deployment.Tenant.Id=#{unless Octopus.Deployment.Tenant.Id}untenanted#{/unless}#{if Octopus.Deployment.Tenant.Id}#{Octopus.Deployment.Tenant.Id | ToLower}#{/if},Octopus.Deployment.Id!=#{Octopus.Deployment.Id | ToLower},FeatureBranch=#{PackagePreRelease}

kubectl delete deployment -l Octopus.Project.Id=#{Octopus.Project.Id | ToLower},Octopus.Environment.Id=#{Octopus.Environment.Id | ToLower},Octopus.Deployment.Tenant.Id=#{unless Octopus.Deployment.Tenant.Id}untenanted#{/unless}#{if Octopus.Deployment.Tenant.Id}#{Octopus.Deployment.Tenant.Id | ToLower}#{/if},Octopus.Deployment.Id!=#{Octopus.Deployment.Id | ToLower},FeatureBranch=#{PackagePreRelease} 
</pre>
<p>
    These calls to <b>kubectl</b> are the same as we used previously, with the addition of one new expression in the
    label selection of <b>FeatureBranch=#{PackagePreRelease}</b>, which restricts the matched resources to those that
    were created as part of this feature branch deployment.
</p>
<p>
    The final change is to update the name of the ingress resources, add the <b>FeatureBranch</b> label, change the name
    of the service they direct traffic to, and configure the HTTP path they match. The YAML for the temporary canary
    ingress resource is:
</p>
<pre class="wrapPre">
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
    name: 'webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}'
    labels:
    FeatureBranch: '#{PackagePreRelease}'
    annotations:
    nginx.ingress.kubernetes.io/canary-weight: '50'
    nginx.ingress.kubernetes.io/canary: 'true'
spec:
    ingressClassName: nginx
    rules:
    - host: '#{Hostname}'
        http:
        paths:
            - path: '#{ContextPath}'
            backend:
                servicePort: web
                serviceName: 'webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}'
    
</pre>
<p>
    The YAML for the main ingress resource is:
</p>
<pre class="wrapPre">
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: 'webapp-canary#{ServiceSuffix}'
  labels:
    FeatureBranch: '#{PackagePreRelease}'
spec:
  ingressClassName: nginx
  rules:
    - host: '#{Hostname}'
      http:
        paths:
          - path: '#{ContextPath}'
            backend:
              servicePort: web
              serviceName: 'webapp-canary#{ServiceSuffix}-#{Octopus.Deployment.Id | ToLower}'
</pre>
<p>
    With these changes in place go ahead and deploy the <b>0.1.9-purpleheader</b> version of the web application. This
    will deploy the container under the subpath of <b>/development-purpleheader</b>, for example http://[reversed ip
    address].bc.googleusercontent.com/development-purpleheader.
</p>
<p>
    Now deploy a mainline version like <b>0.1.9</b>. Because there is no prerelease component to this version, it is not
    considered a feature branch, and the deployment will be exposed on the path of http://[reversed ip
    address].bc.googleusercontent.com/development.
</p>
<p>
    The diagram below shows how a shared load balancer is now distributing traffic to multiple branches:
</p>
<div><img alt="Octopus environments" src="images/diagrams/feature-branches-networking.png" /></div>
<p>
    Also note how the cleanup code doesn't cross the lines between individual feature branches and the mainline
    deployment. We can redeploy the feature branch image over and over, and the mainline deployment is not affected.
</p>
<p class="note">
    To increment the version of a feature branch, you can use versions like <b>0.1.9-purpleheader.1</b>. The identifier
    <b>purpleheader</b> can be extracted with the <b>VersionPreReleasePrefix</b> filter, and the counter on the end of
    <b>1</b> can be extracted with the filter <b>VersionPreReleaseCounter</b>. If we were to deploy an image with the
    version <b>0.1.9-purpleheader.1</b>, it would still be identified as the feature branch <b>purpleheader</b>, and the
    counter on the end of the version can increment with each build.
</p>
<h3>Preventing feature branches from being deployed to production</h3>
<p>
    Feature branches are, by their very nature, deployments of test code under development. They are not intended for
    end users to interact with. But with the feature branch deployments we created above, there is nothing stopping
    anyone from deploying a feature branch into the production environment.
</p>
<p>
    Fortunately Octopus has a solution for this called channels. A channel defines a set of rules that the package
    version must conform to, and any matching packages can then be deployed along a custom lifecycle.
</p>
<p>
    We start by creating an additional lifecycle with only the <b>Development</b> and <b>Test</b> environments. Open
    <b>Library -> Lifecycles</b>, and using the same process that we followed in <a href="#chapter1">chapter 1</a>
    create a lifecycle called <b>Dev and Test</b> with two phases called <b>Development</b> and <b>Test</b> that contain
    the environment of the same name:
</p>
<div><img alt="Octopus environments" src="images/octopus/feature-branch-lifecycle.png" /></div>
<p>
    Back in our deployment project, click the <b>Channels</b> link. Each project has a <b>Default</b> lifecycle that
    inherits the lifecycle assigned to the deployment process. In our case this lifecycle allows deployments to be
    progressed all the way to production. We want to ensure that only images without a prerelease version component, for
    example versions like <b>0.1.9</b>, can be deployed through this lifecycle.
</p>
<p>
    Click the <b>Default</b> channel, and click the <b>ADD VERSION RULE</b> button. Select the <b>Deploy Web App</b>
    step, and enter <b>^$</b> for the pre-release tag. This string is a regular expression that only matches an empty
    string, which means it only matches versions like <b>0.1.9</b>.
</p>
<p>
    Save the changes, and return to the list of channels again. Click the <b>ADD CHANNEL</b> button to add a new channel
    rule. Give it the name <b>Feature Branch</b> and select the <b>Dev and Test</b> lifecycle.
</p>
<p>
    Click the <b>ADD VERSION RULE</b> button. Select the <b>Deploy Web App</b> step, and enter <b>.+</b> for the
    pre-release tag. This is a regular expression that matches any character one or more times, which means it only
    matches versions like <b>0.1.9-purpleheader</b>. Click <b>SAVE</b> to save the changes.
</p>
<p class="note">
    Because the tag <b>latest</b> has a prerelease component (actually the entire tag is the prerelease), it will match
    the regex <b>.+</b>. To match all tags with a prerelease component except <b>latest</b>, use the regex
    <b>^(?!latest\b).+$</b>.
</p>
<p>
    Now create a new release, select the <b>Feature Branch</b> channel, and select the image version
    <b>0.1.9-purpleheader</b>. Note that when you browse for image versions, mainline versions like <b>0.1.9</b> are not
    available. Deploy the release to the <b>Development</b> environment.
</p>
<p>
    Return to the <b>Overview</b> page, and notice that the feature branch release can only be promoted from the
    <b>Development</b> environment to the <b>Test</b> environment. There is no ability to deploy it into the
    <b>Production</b> environment:
</p>
<div><img alt="Octopus environments" src="images/octopus/channel-dashboard.png" /></div>
<p>
    Now go ahead and create a release in the <b>Default</b> channel. Notice that you are unable to select versions like
    <b>0.1.9-purpleheader</b>. This means only mainline versions can be deployed in the <b>Dev, Test, and Prod</b>
    lifecycle, and therefor only mainline versions can be deployed into the <b>Production</b> environment.
</p>
<p>
    We have created feature branch deployment strategy that prevents feature branches from being deployed to production.
    This gives developers the ability to test their changes in isolation, safe in the knowledge that their test code
    won't be exposed to end users.
</p>
<h3>Cleaning up old feature branches</h3>
<p>
    Feature branches tend to be short lived, so we would expect that any feature branch deployments to be cleaned up
    when they are no longer useful. To accommodate this we'll provide a runbook to delete feature branch resources.
</p>
<p>
    Inside the deployment project, click the <b>Operations -> Runbooks</b> like, and click the <b>ADD RUNBOOK</b>
    button. Add a runbook called <b>Delete Feature Branch</b>.
</p>
<p>
    Add a <b>Run a kubectl CLI Script</b> step to the runbook process, target the <b>Frontend</b> role, and run the step
    in an appropriate container image. The script step is set to the following:
</p>
<pre class="wrapPre">
kubectl delete ingress -l FeatureBranch=#{FeatureBranch}

kubectl delete service -l FeatureBranch=#{FeatureBranch}

kubectl delete deployment -l FeatureBranch=#{FeatureBranch}
</pre>
<p>
    Save the runbook, and then in the <b>Settings</b> tab limit the runbook to the <b>Development</b> and <b>Test</b>
    environments, as we don't expect to be deploying feature branches to production.
</p>
<p>
    In the project variables, create a new variable called <b>FeatureBranch</b>. Click the <b>Enter value</b> field,
    select <b>OPEN EDITOR</b>, click the <b>Prompt for value</b> and <b>Required</b> options, and click the <b>DONE</b>
    button. Then click the <b>Define scope</b> field and select <b>Delete Feature Branch</b> from the <b>Select
        processes</b> drop down list.
</p>
<p>
    This variable will now prompt for a value when the runbook is run, but because it is scoped to only the <b>Delete
        Feature Branch</b> process, the main deployment will not prompted for a variable:
</p>
<div><img alt="Octopus environments" src="images/octopus/prompted-variable-1.png" /></div>
<p>
    Running this runbook will remove any resources associated with a feature branch, effectively undeploying a feature
    branch from an environment.
</p>
<h2>Conclusion</h2>
<p>
    As your deployments increase in frequency, it is desirable to perform them with little or no downtime for end users.
    Progressively rolling out deployments to more end users can also be used to limit the blast radius of any errors
    that you may have inadvertently introduced into your production environment. And running feature branches side by
    side with the main deployments is a convenient way for developers to test changes before they are merged into the
    main code base.
</p>
<p>
    In this chapter we:
</p>
<ul>
    <li>Listed the common deployment strategies used today, noting those that apply to Kubernetes.</li>
    <li>Provided examples implementing rolling, recreate, blue/green, canary, and feature branch deployments in Octopus.
    </li>
    <li>Installed the NGINX ingress controller to implement canary deployments.</li>
    <li>Use channels and lifecycles to prevent test code from being deployed to production.</li>
    <li>Added a runbook, collocated with the deployment process, to clean up old feature branches.</li>
</ul>
<p>
    It can take a bit of work to build up the more complex deployment patterns like canary deployments, but as we'll see
    the these deployment patterns provide a nice foundation as we move into the next pillar of recoverable deployments.
</p>