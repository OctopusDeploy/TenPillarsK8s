<h1><a id="chapter3">Seamless deployments</a></h1>
<p>
    Reducing or eliminating downtime during a deployment becomes increasingly critical as the number and frequency of deployments increases. There are a number of common deployment strategies to reduce downtime or ensure our applications remain available throughout a deployment, resulting in seamless deployments. Given the range of high quality networking solutions that are available in the Kubernetes ecosystem, Kubernetes deployments are well placed to take advantage of these strategies.
</p>
<p>
    In this chapter we'll look at the common deployment strategies and provide examples of how these strategies can be realized with an Octopus deployment.
</p>
<h2>Dealing with databases during deployments</h2>
<p>
    No discussion on seamless deployments can begin without first addressing the issue of database updates.
</p>
<p>
    A fundamental aspect of most seamless deployment strategies involves running two versions of your application side by side, if only for a short period of time. If both versions of the application access a shared database, then any updates to the database schema and data must be compatible with both application versions. This is referred to as backward and forward compatibility.
</p>
<p>
    However, backward and forward compatibility is not trivial to implement. In the presentation Update your Database Schema with Zero Downtime Migrations available at <a href="https://www.youtube.com/watch?v=3mj6Ni7sRN4">https://www.youtube.com/watch?v=3mj6Ni7sRN4</a> (based on chapter 3 of the book Migrating to Microservice Databases available at <a href="https://developers.redhat.com/books/migrating-microservice-databases-relational-monolith-distributed-data">https://developers.redhat.com/books/migrating-microservice-databases-relational-monolith-distributed-data</a>), Edison Yanaga walks through the process of renaming a single column in a database. It involves six incremental updates to the database and application code, and all six versions to be deployed sequentially.
</p>
<p>
    Needless to say, seamless deployments involving databases require a great deal of planning, many small steps to roll out the changes, and tight coordination between the database and application code.
</p>
<p>
    We won't dive into the specifics of how to engineer an application with forward and backward database compatibility in this book. This is an application design concern, and whether or not an application supports forward and backward database compatibility is already determined when it is to be deployed. The deployment strategy does need to take into account whether an application supports running two versions side by side, but can not add this functionality if it isn't already built into the application.
</p>
<h2>Common deployment strategies</h2>
<p>
    There are multiple strategies to manage a cutover between an existing deployment and a new one.
</p>
<h3>Recreate</h3>
<p>
    The recreate strategy does not provide a seamless deployment, but is included here as it is the default option for most deployment processes. This strategy involves either removing the existing deployment and deploying the new version, or deploying the new version over the top of the existing deployment.
</p>
<p>
    Both options result in downtime during the period between the existing version being stopped or removed and the new version being started. However, because the existing and new versions are not run concurrently, database upgrades can be applied as needed with no backward and forward compatibility requirements.
</p>
<h3>Rolling updates</h3>
<p>
    The rolling update strategy involves incrementally updating instances of the current deployment with the new deployment. This strategy ensures there is always at least one instance of the current or new deployment available during the rollout. This requires that any shared database must maintain backward and forward compatibility.
</p>
<h3>Canary deployments</h3>
<p>
    The canary deployment strategy is similar to the rolling update strategy in that both incrementally expose more end users to the new deployment. The difference is that the decision to progress the rollout in a canary deployment is either made automatically by a system monitoring metrics and logs to ensure the new deployment is performing as expected, or manually by a human.
</p>
<p>
    Canary deployments also have the option to halt the rollout and revert back to the existing deployment if a problem is discovered.
</p>
<h3>Blue/green deployments</h3>
<p>
    The blue/green strategy involves deploying the new version (referred to as the green version) alongside the current version (referred to as the blue version), without exposing the green version to any traffic. Once the green version is deployed and verified, traffic is cutover from the blue to the green version. When the blue version is no longer used, it can be removed.
</p>
<p>
    Any database changes deployed by the green version must maintain backward and forward compatibility, because even if the green version is not serving traffic, the blue version will be exposed to the database changes.
</p>
<h3>Session draining</h3>
<p>
    The session draining strategy is used when applications maintain states tied to a particular application version.
</p>
<p>
    This strategy is similar to the blue/green strategy in that both will deploy the new version alongside the current version, and run both side by side. Unlike the blue/green strategy, which will cut all traffic over to the new deployment in a single step, the session draining strategy will direct new sessions to the new deployment, while the existing deployment serves traffic to existing sessions.
</p>
<p>
    After the old sessions have expired, the existing deployment can be deleted.
</p>
<p>
    Because the current and new deployments run side by side, any database changes must maintain backward and forward compatibility.
</p>
<p>
    Session draining was particularly important for older server side stateful web applications. This style of application has largely gone out of style, in large part because it made it difficult to manage with PaaS offerings that often treated any individual application instance as ephemeral. This book won't cover session draining in Kubernetes clusters.
</p>
<h3>Feature flags</h3>
<p>
    The feature flag strategy involves building functionality into a new application version, and then exposing or hiding the feature for select end users outside of the deployment process.
</p>
<p>
    In practice, the deployment of a new application version with flaggable features will be performed with one of the strategies above, so the feature flag strategy is a complement to those other strategies.
</p>
<p>
    A more limited approach to feature flags may be to enable or disable features for an entire deployment. For example, a new feature may be exposed to testers in non-production environments, but be disabled entirely when deployed to production. This broader approach is something that can be managed as part of a deployment.
</p>
<h3>Feature branch</h3>
<p>
    The feature branch strategy allows developers to deploy an application version with changes they are currently implementing, usually in a non-production environment, alongside the main deployment.
</p>
<p>
    It may not be necessary to maintain database backward and forward compatibility with feature branch deployments. Because feature branches are for testing and tend to be short-lived, it may be acceptable that each feature branch deployment has access to its own test database.
</p>
<h2>Seamless {{ site.platform }} deployments</h2>
<p>
    Kubernetes has out of the box support for the rolling deployment strategy for pods managed by a deployment, statefulset, or daemonset. Kubernetes also supports the recreate strategy for pods managed by a deployment. This gives us tick box support to utilize these strategies while deploying our applications.
</p>
<p>
    More advanced deployment strategies like blue/green or canary will often require some planing during a deployment, or rely on third party networking tools to provide fine grained control over which services respond to a request.
</p>
<p>
    Kubernetes has no concept of feature branch deployments. However, with some sensible naming patterns it is possible to implement feature branch deployments in a relatively straightforward manner.
</p>
<h3>Rolling and recreate deployments</h3>
<p>
    Although we didn't call it out in <a href="chapter1">chapter 1</a>, the deployment process we created is already configured to perform a rolling update. All Kubernetes deployment resources will be updated using either a rolling or recreate strategy, and the rolling update strategy was selected for us by default.
</p>
<p>
    To make either strategy meaningful we need to deploy more than one pod, so open the <b>Random Quotes - Frontend</b> project, select the <b>Deploy Kubernetes containers</b> step that deploys the frontend application, and then change the replicas count to 5. This will result in 5 pods being created with the frontend web application, with traffic distributed between them by the service.
</p>
<p>
    Deploy the <b>Random Quotes - Frontend</b> project to the <b>Development</b> environment with the new replica count to populate the cluster with the new pods.
</p>
<p>
    Now redeploy the <b>Random Quotes - Frontend</b> project to the <b>Development</b> environment. When Octopus presents the package versions, expand the packages section, click the <b>SELECT VERSION</b> button next to the <b>octopussamples/randomquotesjava</b> package, tick the <b>Pre-release packages</b> option, and select the <b>0.1.9-purpleheader</b> version. This version of the image replaces the background color of the header displayed by the web frontend. Click the <b>OK</b> button to select the new version.
</p>
<p class="note">
    Although Docker tags have no concept of a pre-release, Octopus treats tags like <b>0.1.9-purpleheader</b> as a prerelease version, with the identifier <b>purpleheader</b> indicating this is a pre-release.
</p>
<p>
    The log files generated during the deployment show entries like <b>Waiting for deployment "webapp" rollout to finish: 4 out of 5 new replicas have been updated...</b>, followed by entries like <b>Waiting for deployment "webapp" rollout to finish: 2 old replicas are pending termination...</b>. This indicates that Kubernetes is deploying pods with the new image before deleting pods with the old image.
</p>
<p>
    The diagram below shows how a rolling update strategy progresses, with old pods progressively deleted and replaced with new pods:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/rolling-deployment.png"/></div>
<p>
    Go back to the deployment process for the <b>Random Quotes - Frontend</b> project, select the <b>Deploy Kubernetes containers</b> step that deploys the frontend application, expand the <b>Deployment Strategy</b> section, and select the <b>Recreate deployments</b> option. Save the changes and redeploy the project to the <b>Development</b> environment.
</p>
<p>
    Notice now the log files now only have messages like <b>Waiting for deployment "webapp" rollout to finish: 0 out of 5 new replicas have been updated...</b>, with the replica count increasing until all pods have been deployed. This indicates that the old pods were removed before any new pods were deployed.
</p>
<p>
    The diagram below shows how a recreate update strategy progresses, where all old pods are first deleted, and new pods are then deployed:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/recreate-deployment.png"/></div>
<h3>Blue/green deployments</h3>
<p>
    Unlike the rolling and recreate strategies, blue/green deployments are not natively supported by Kubernetes. However, Octopus does provide the ability to perform blue/green deployments by orchestrating the various Kubernetes resources for you.
</p>
<p>
    Open the <b>Random Quotes - Frontend</b> project, select the <b>Deploy Kubernetes containers</b> step that deploys the frontend application, expand the <b>Deployment Strategy</b> section, and select the <b>Blue/Green deployments</b> option. Save the changes and deploy the project to the <b>Development</b> environment.
</p>
<p>
    Notice that the logs container entries like <b>deployment.apps/webapp-deployments-101 created</b>. Of particular note is that the name of the deployment is made up of the name that was entered into the step, which was <b>webapp</b>, and a suffix like <b>-deployments-101</b>.
</p>
<p>
    This is a crucial aspect of cloud deployments. To have the blue and green stacks coexist side by side in the cluster, resources like the Kubernetes deployment must have unique names. Octopus ensures this by appending the deployment ID to the end of the deployment name. This ensures each deployment orchestrated by Octopus generates a unique Kubernetes deployment resource.
</p>
<p>
    Note though that not all resources created by this step have a unique name. There is only one service created as part of a blue/green deployment, so it retains the name of <b>webapp</b>.
</p>
<p>
    After the first blue/green deployment, our cluster looks like this. We have a shared service pointing to a uniquely named deployment. The deployment was successful, so the new deployment becomes the blue stack:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen1.png"/></div>
<p>
    Now lets deploy the project to the <b>Development</b> environment again. Notice now the logs container entries like <b>deployment.apps/webapp-deployments-102 created</b>. This is a new deployment, so the suffix appended to the deployment resource has been updated.
</p>
<p>
    As the new deployment resource is spun on in the cluster, the state of the cluster looks like this. The new deployment, considered to be the blue stack, is created but not yet receiving traffic:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen2.png"/></div>
<p>
    Once the new deployment resource has passed all of its health checks and is considered fully available, the service is updated to direct traffic to the it:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen3.png"/></div>
<p>
    The resources in the blue stack are now deleted:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen4.png"/></div>
<p>
    With the deployment process now done, the resources from the green stack are now considered to be the blue stack:
</p>
<div><img alt="Octopus environments" src="images/kubernetes/bluegreen5.png"/></div>
<p>
    So, in summary, these are the steps Octopus will orchestrate for a blue/green deployment:
</p>
<ol>
    <li>Create a Kubernetes deployment resource with a unique name in the green stack.</li>
    <li>Wait for the new deployment to pass its health checks and be available.</li>
    <li>Update a shared service to point to the new deployment resource.</li>
    <li>Delete the old deployment resources in the blue stack.</li>
    <li>At the end of the deployment, the green stack is considered to now be the blue stack.</li>
</ol>
<h3>The opinions embedded in the Deploy Kubernetes containers step</h3>
<p>
    While performing a blue/green deployment we saw that Octopus will ensure that certain resources have unique names by appending the ID of the Octopus deployment to the end. In the case of a deployment resource, this is required to allow the green and blue stacks to coexist while traffic is cut over from the green stack to the blue stack.
</p>
<p>
    Any secrets, configmaps, or custom resources defined in the <b>Deploy Kubernetes containers</b> step also have their names altered to ensure they are unique, regardless of the deployment strategy.
</p>
<p class="note">
    A custom resource is any Kubernetes resource that can be defined in YAML. The ability to define a customer resource is enabled by clicking the <b>CONFIGURE FEATURES</b> button and selecting the <b>Custom Resource</b> option.
</p>
<p>
    Providing unique names for these resources is one of the opinions that is baked into the <b>Deploy Kubernetes containers</b> step. The step assumes that these resources exist to support the main deployment resource, and that they are tightly bound to the lifecycle of any pods managed by the deployment.
</p>
<p>
    For example, if a configmap is defined as part of the <b>Deploy Kubernetes containers</b> step, it is assumed that the data in the configmap is required for the pods managed by the deployment to run correctly. This might be because the configmap defines values that are exposed as pod environment variables, or mounted as files.
</p>
<p>
    By ensuring the configmap has a unique name for each deployment, we can be sure that during a rolling or blue/green update, the old pods have access to the data in the old configmaps, and the new pods have access to the data in the new configmaps. If both the old and the new pods had shared a single configmap, it would be possible that any new data in the shared configmap would break the old pods in unexpected ways. 
</p>
<p>
    Resources created by all other Kubernetes steps in Octopus do not alter the resource's names. So if for example you need to create a configmap that is shared between all pods, you can use the <b>Deploy Kubernetes config map resource</b> step, which will create a configmap with the name supplied in the step with no modification.
</p>
<h3>Preparing for a canary deployment</h3>
<p>
    Octopus managed the blue/green deployment for us with a hard cutover from the green to the blue stack once the blue stack was ready. Canary deployments are different in that only a subset of traffic is directed to the new blue stack. The blue stack traffic is incrementally increased until eventually all traffic has migrated from the green stack to the blue stack.
</p>
<p>
    The networking features native to Kubernetes do not directly support this kind of graduated traffic increase. It would technically be possible to direct more traffic to the green stack through a service that selected pods from both the blue and green stacks, decreasing the blue stack pod count, and increasing the green stack pod count. But the traffic percentage changes are limited by the number of pods, making it a rather coarse solution.
</p>
<p>
    Fortunately Kubernetes has a rich ecosystem of third party networking tools that specifically support canary deployments. One popular networking solution is the NGINX ingress controller.
</p>
<p class="note">
    Kubernetes services coordinate layer 4 network traffic (i.e. TCP and UDP traffic) amongst pods. Ingress controllers provide an additional layer to direct layer 7 traffic (i.e. HTTP traffic) to services. Kubernetes provides a standard ingress resource, but does not supply an ingress controller to implement the ingress rules. Third party ingress controllers, like NGINX, are installed into a Kubernetes cluster to implement ingress rules.
</p>
<p>
    The easiest way to install tools like NGINX in a Kubernetes cluster is with Helm. Helm is like an operating system package manager, such as Chocolatey for Windows or APT for Ubuntu, with the obvious difference being Helm is used to deploy applications into a Kubernetes cluster.
</p>
<p>
    To consume Helm packages, called charts, we need to add a Helm external feed. Open <b>Library -> External Feeds</b>, and click the <b>ADD FEED</b> button. Select <b>Helm Feed</b> as the feed type, and give the feed the name of <b>Helm</b>. The default URL of https://charts.helm.sh/stable is fine, so click the <b>SAVE</b> button.
</p>
<p>
    We'll then deploy the NGINX ingress controller through a new runbook in the <b>Kubernetes DevOps</b> project called <b>Deploy NGINX Ingress Controller</b>. Add the <b>Upgrade a Helm Chart</b> step to the runbook.
</p>
<p>
    This step will run on behalf of targets in the <b>Kubernetes Admin</b> role. The step will also need access to the <b>helm</b> command line tool, which we can access by using the <b>octopusdeploy/worker-tools:windows.ltsc2019</b> or <b>octopusdeploy/worker-tools:ubuntu.18.04</b> container images.
</p>
<p>
    Select the <b>Helm</b> feed in the chart section, and enter <b>nginx-ingress</b> as the package id. Finally enter <b>nginx-ingress</b> as the Kubernetes release name.
</p>
<p>
    All the other default values in the step are fine for us, so save the step, and run the runbook in the <b>Admin</b> environment.
</p>
<p>
    The benefit of using Helm to deploy third party applications like NGINX is that a Helm chart bundles all the various resources needed for that application to function. One of the resources that we are interested in is the load balancer service created to direct public network traffic into the ingress controller. This load balancer service is much the same as we created in <a href="chapter1">chapter 1</a>, except that the traffic is now sent to an ingress controller that can be configured with fine grained rules to route HTTP traffic amongst many internal services.
</p>
<p>
    As before, we need a way to get the public IP address of this load balancer service. Luckily we already have the <b>Get Service IP</b> runbook from <a href="chapter1">chapter 1</a>. By running this runbook in the <b>Admin</b> environment, we can get the IP address of the NGINX load balancer service. The <b>EXTERNAL-IP</b> assigned to the <b>nginx-ingress-controller</b> service provides the value we need:
</p>
<div><img alt="Octopus environments" src="images/octopus/nginx-loadbalancer.png"/></div>
<p>
    We now have an ingress controller to split our canary traffic between an existing and a new deployment and its service. The next step is to deploy the frontend web application to support canary deployments.
</p>
<h3>Canary deployments</h3>
<p>
    While the blue/green deployment strategy was easy to implement thanks to the fact that all it took selecting an single option in the <b>Deploy Kubernetes containers</b> step, it has the downside that the entire process is self contained and not customizable. The process it implements is very similar to what we want to achieve in a canary deployment though. So we'll go ahead and recreate the blue/green deployment process as individual steps in a deployment process. 
</p>
<p>
    If you recall from the previous section, a blue/green deployment performs the following steps:
</p>
<ol>
    <li>Create a Kubernetes deployment resource with a unique name in the green stack.</li>
    <li>Wait for the new deployment to pass its health checks and be available.</li>
    <li>Update a shared service to point to the new deployment resource.</li>
    <li>Delete the old deployment resources in the blue stack.</li>
    <li>At the end of the deployment, the green stack is considered to now be the blue stack.</li>
</ol>
<p>
    Performing a canary deployment is a little more complicated, but shares many of those steps. Our canary deployment will follow this process:
</p>
<ol>
    <li>Create a Kubernetes deployment resource with a unique name in the green stack.</li>
    <li>Wait for the new deployment to pass its health checks and be available.</li>
    <li>Create a unique service to direct traffic to the pods in the green stack.</li>
    <li>Create an temporary ingress to direct traffic to the green stacks.</li>
    <li>Incrementally increase the traffic directed to the green stack until it reaches 100%.</li>
    <li>(Re)create the main ingress to point traffic to the green stack.</li>
    <li>Delete the temporary ingress.</li>
    <li>Delete the old deployment resources in the blue stack.</li>
    <li>At the end of the deployment, the green stack is considered to now be the blue stack.</li>
</ol>
<p>
    Let's create a new project called <b>Random Quotes - Frontend - Canary</b>. Go to the process editor and add a new <b>Deploy Kubernetes containers</b> step.
</p>
<p>
    Import the following YAML in the <b>Deployment</b> section:
</p>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        octopusexport: OctopusExport
    spec:
      containers:
        - name: webapp
          image: index.docker.io/octopussamples/randomquotesjava
          ports:
            - name: web
              containerPort: 5555
          env:
            - name: SPRING_CONFIG_NAME
              value: postgres-application
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_URL
              value: 'jdbc:postgresql://postgres.backend-development:5432/postgres'
            - name: POSTGRES_PASS
              value: '#{DatabasePassword}'
          readinessProbe:
            failureThreshold: 6
            periodSeconds: 20
            tcpSocket:
              host: ''
              port: 5555
</pre>
<p>
    Notice the deployment name is set to <b>webapp-canary-#{Octopus.Deployment.Id | ToLower}</b>. Generating a unique name with each deployment replicates the functionality that Octopus provides automatically when performing a blue/green deployment.
</p>
<p>
    Otherwise the configuration of this deployment is the same as we configured previously.
</p>
<p>
    Under the <b>Service</b> section import the following YAML:
</p>
<pre>
apiVersion: v1
kind: Service
metadata:
  name: 'webapp-canary-#{Octopus.Deployment.Id | ToLower}'
spec:
  type: ClusterIP
  ports:
    - name: web
      port: 80
      targetPort: 5555
      protocol: TCP
  selector:
    octopusexport: OctopusExport
</pre>
<p>
    Again we have given this service a unique name, ensuring each time we deploy this project with Octopus a new service will be created.
</p>
<p>
    The next steps recreate the smoke, integration, and end-to-end tests that we introduced in <a href="chapter2">chapter 2</a>. The steps are the same when performing a canary deployment, with the only thing that needs to change is the URL that each of the tests contacts, which has changed now to http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-development thanks to the unique name now assigned to the service with each deployment.
</p>
<p>
    Making this change in the smoke tests is easy, as we implemented those with inline Bash scripts, and the URL can be updated im place by updating the script. But how do you go ahead and update in the Postman and Cypress scripts we used for the integration and end-to-end tests?
</p>
<p>
    To achieve this, we'll make use a feature in Octopus called <b>Structured Configuration Variables</b>, which will inject values into structured data files such as YAML, JSON, XML, or Java Properties files.
</p>
<p>
    Let's start with the Postman collection JSON file. Because this test uses a format recognized by the <b>Structured Configuration Variables</b> feature, we can reuse the existing test and modify it at runtime to suit our needs.
</p>
<p>
    Here is the JSON file again:
</p>
<pre>
{
    "info": {
        "_postman_id": "2c6a5ce5-e2af-4f23-ab6c-172d0dfa3797",
        "name": "RandomQuotes",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Quotes API",
            "event": [
                {
                    "listen": "test",
                    "script": {
                        "exec": [
                            "pm.test(\"response must be valid and have a body\", function () {\r",
                            "     pm.response.to.be.ok;\r",
                            "     pm.response.to.be.withBody;\r",
                            "     pm.response.to.be.json;\r",
                            "\r",
                            "     pm.expect(pm.response.json().quote != \"\").to.be.true;\r",
                            "});"
                        ],
                        "type": "text/javascript"
                    }
                }
            ],
            "request": {
                "method": "GET",
                "header": [],
                "url": {
                    "raw": "http://webapp.frontend-development/api/quote",
                    "protocol": "http",
                    "host": [
                        "webapp",
                        "frontend-development"
                    ],
                    "path": [
                        "api",
                        "quote"
                    ]
                }
            },
            "response": []
        }
    ]
}
</pre>
<p>
    To edit this file to make it useful for our new service URL we need to update the <b>item[0].request.url.raw</b> property from <b>http://webapp.frontend-development/api/quote</b> to <b>http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-development</b>.
</p>
<p>
    The syntax we used to create the property <b>item[0].request.url.raw</b> will likely be familiar to developers, but it can be strange if you do not write code for a living.
</p>
<p>
    The property is broken down by periods into four items: <b>item[0]</b>, <b>request</b>, <b>url</b>, and <b>raw</b>. The first element of <b>item</b> refers to the array of the same name in the JSON file. The square brackets of <b>[0]</b> indicates that we are selecting the first item in the array. 
</p>
<p class="note">
    JSON arrays are identified by square brackets. Arrays use a zero based index, meaning the index of zero references the first item.
</p>
<p>
    We then continue to traverse the JSON tree. The <b>request</b> element selects the object of the same name inside the first element of the <b>item</b> array. We then select the <b>url</b> object, and finally the <b>raw</b> property.
</p>
<p class="note">
    JSON objects are identified by curly brackets.
</p>
<p>
    We also need to update the <b>item[0].request.url.host[0]</b> property to <b>webapp-canary-#{Octopus.Deployment.Id | ToLower}</b>, and <b>item[0].request.url.host[1]</b> to <b>frontend-#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    To configure Octopus to make these changes for us we need to create three variables. The first will have the name <b>item[0].request.url.raw</b> and the value <b>http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}</b>. The second will have the name <b>item[0].request.url.host[0]</b> and the value <b>webapp-canary-#{Octopus.Deployment.Id | ToLower}</b>. The third will have the name <b>item[0].request.url.host[1]</b> and the value <b>frontend-#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    Then in the script step that is running the integration test, click the <b>CONFIGURE FEATURES</b> button, select the <b>Structured Configuration Variables</b> option, and save the changes.
</p>
<p>
    This will display a new section in the step called <b>Structured Configuration Variables</b>. We then list the files that we want Octopus to process in the target files text box. We can enter <b>IntegrationTest/*.json</b> to have Octopus attempt to inject any the values of the two variables we created earlier into any JSON file found from the <b>IntegrationTest</b> additional package reference. The wildcard syntax means our file called <b>RandomQuotes.postman_collection.json</b> will be matched and processed.
</p>
<p>
    We can perform a similar manipulation in the script step running the end-to-end test. Edit that step to enabled the <b>Structured Configuration Variables</b> feature, and then specify the file <b>cypress/cypress.json</b> in the target files text box.
</p>
<p>
    If you recall from <a href="#chapter2">chapter 2</a> the <b>cypress.json</b> file looks like this:
</p>
<pre>
{
    "baseUrl": "http://webapp.frontend-development",
    "reporter": "mochawesome",
    "reporterOptions": {
        "charts": true,
        "overwrite": false,
        "html": true,
        "json": false,
        "reportDir": "."
    }
}
</pre>
<p>
    We want to update the <b>baseUrl</b> property to  <b>http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-development</b>. So add an Octopus variable called <b>baseUrl</b> with the value of <b> http://webapp-canary-#{Octopus.Deployment.Id | ToLower}.frontend-#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    By using the <b>Structured Configuration Variables</b> options in our integration and end-to-end test scripts we can take test scripts generated by external tools with values that are not specific to our infrastructure and modify them at deployment time to suit our needs. This pattern allows us to write test scripts that work on a local PC (for example targeting URLs like http://localhost) and use them across multiple environments. Doing so frees the test writers from providing multiple test packages to support each environment, or even from knowing the details of the environments that the tests will eventually be run in. We take care of this at deployment time.
</p>
<p>
    Once the tests pass we will direct a subset of incoming traffic to the canary stack (or green stack in keeping with the terminology we have already used in blue/green deployments) via an ingress.
</p>