<h1><a id="chapter2">Verifiable deployments</a></h1>
<p>
    The repeatable deployments pillar describes how promoting releases through environments provides an increasing level of confidence in the solution being delivered. We talked about how frequent deployments to the development environment enabled developers to test their changes, while less frequent deployments to the test environment allowed other parties to verify a potential production release. When everyone is happy, the production environment is updated, exposing the changes to end users.
</p>
<p>
    The pillar of verifiable deployments describes the various techniques that can be used to verify a deployment when it reaches a new environment.
</p>
<h2>General testing concepts</h2>
<p>
    Testing is a nebulous term with often ill-defined subcategories. We will not attempt to provide authoritative definitions of testing categories here. Our goal is to offer a very high level description of common testing practices, and highlight those that can be performed during the deployment process.
</p>
<h2>What don't we test during deployments?</h2>
<p>
    Unit tests are considered part of the build pipeline. These tests are tightly coupled to the code being compiled, and they must succeed for the resulting application package to be built.
</p>
<p>
    Integration tests may also be run during the build process to verify that higher level components interact as expected. The components under test may be replaced with a test double to improve reliability, or live instances of the components may be created as part of the test.
</p>
<p>
    Unit and integration tests are run by the CI server, and any package that is made available for deployment is assumed to have passed all its associated unit and integration tests.
</p>
<h2>What can we test during deployment?</h2>
<p>
    Tests that require a live application or application stack to be accessible are ideal candidates to be run as part of a deployment process.
</p>
<p>
Smoke tests are quick tests designed to ensure that applications and services have deployed correctly. Smoke tests implement the minimum interaction required to ensure services respond correctly. Some examples include:
</p>
<ul>
    <li>An HTTP request of a web application or service to check for a successful response.</li>
    <li>A database login to ensure the database is available.</li>
    <li>Checking that a directory has been populated with files.</li>
    <li>Querying the infrastructure layer to ensure the expected resources were created.</li>
</ul>
<p>
    Integration tests can be performed as part of a deployment as well as during the build. Integration tests validate that multiple components are interacting as you expect. Test doubles may be included with the deployment to stand in for components being verified, or the tests may verify two or more live component instances. Examples include:
</p>
<ul>
    <li>Logging into a web application to verify that it can interact with an authentication provider.</li>
    <li>Querying an API for results from a database to ensure that the database is accessible via a service.</li>
</ul>
<p>
    End to end tests provide an automated way of interacting with a system in the same way a user would. These can be long running tests following paths through the application that require most or all components of the application stack to be working correctly. Examples include:
</p>
<ul>
    <li>Automating the interaction with an online store to browse a catalog, view an item, add it to a cart, complete the checkout, and review the account order history.</li>
    <li>Completing a series of API calls to a weather service to find a city's latitude and longitude, getting the current weather for the returned location, and returning the forecast for the rest of the week.</li>
</ul>
<p>
    Chaos testing involves deliberately removing or interfering with the components that make up an application to validate that the system is resilient enough to withstand such failures. Chaos testing may be combined with other tests to verify the stability of a degraded system.
</p>
<p>
    Usability and acceptance testing typically require a human to use the application to verify that it meets their requirements. The requirements can be subjective, for example, determining if the application is visually appealing. Or the testers may not be technical, and so do not have the option of automating tests. The manual and subjective nature of these tests makes them difficult, if not impossible, to automate, meaning a working copy of the application or application stack must be deployed and made accessible to testers.
</p>
<h2>Verifiable {{ site.platform }} deployments</h2>
<p>
    The first practical consideration when testing deployments is where the tests are run.
</p>
<p>
    If you recall from <a href="#chapter1">chapter one</a>, the service we created to expose the database was of type <b>ClusterIP</b>, which means that the service only exposes a private IP address that can be accessed from within the Kubernetes cluster.
</p>
<p>
    We could expose the database to public traffic, but this is an unnecessary security risk given the only clients accessing the database (i.e. the frontend web application) are also deployed in the same cluster.
</p>
<p>
    It then follows that any test requiring access to the database will need to be run within the cluster.
</p>
<p>
    Just as workers can be placed on a cloud VM with access to a private network, workers can be run as containers inside a Kubernetes cluster. While traditional tentacles and workers are installed directly on a host operating system, tentacles are also provided as Docker images called <b>octopusdeploy/tentacle</b> hosted on Docker Hub.
</p>
<p>
    To create workers in a Kubernetes cluster, we'll need to define a new worker pool, and a new machine policy that automatically cleans up any unavailable workers.
</p>
<h3>Tools used in this chapter</h3>
<p>
    This chapter will make use of a number of third party tools that you will need to install to follow along with the exercise including:
</p>
<ul>
    <li>Docker, downloaded from <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
    <li>Node package manager (npm), installed from <a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a>.</li>
    <li>Postman, installed from <a href="https://www.postman.com/downloads/">https://www.postman.com/downloads/</a>.</li>
</ul>
<p>
    This chapter also uses Docker Hub as a Docker image repository. You can sign up to Docker Hub at <a href="https://hub.docker.com/signup">https://hub.docker.com/signup</a>.
</p>
<h3>Preparing the testing workers in Octopus</h3>
<p>
    We start by creating a new worker pool to hold the workers that will be used for testing. Click <b>Infrastructure -> Worker Pools</b> and click the <b>ADD WORKER POOL</b> button. Give the pool the name of <b>Testing</b>, leave it as the default type of <b>Static</b>, and click the <b>SAVE</b> button.
</p>
<p>
    Pods in a Kubernetes cluster, and the containers they create, are typically considered ephemeral. This means we shouldn't rely on any particular container to run for any length of time. So when running workers in a Kubernetes cluster, we need some way to clean up workers that have been deleted or replaced.
</p>
<p>
    This is where machine policies come in. A machine policy defines the behavior of a tentacle or worker, including how frequently the health and availability of the tentacle is checked, and what to do if a tentacle is unavailable. In our case, we want unavailable workers to be removed from Octopus automatically.
</p>
<p>
    To create a new machine policy, click <b>Infrastructure -> Machine Policies</b>, and then click <b>ADD MACHINE POLICY</b>. Give the policy a name of <b>Test Workers</b>.
</p>
<p>
    We want to test the workers relatively frequently, so under the <b>Health Checks</b> section set the <b>Time Between Checks</b> field to 10 minutes.
</p>
<p>
    We then want any unavailable workers to be deleted from within Octopus, so under the <b>Cleaning Up Unavailable Deployment Targets</b> section set the <b>Behavior</b> option to <b>Automatically delete unavailable machines</b>, and set the time to 30 minutes.
</p>
<p>
    Combined these settings ensure that each worker is contacted by Octopus every 10 minutes to ensure they are available. Unavailable workers will not be selected from the worker pool. Every 30 minutes Octopus will delete unavailable workers, effectively cleaning up the pool and ensuring old workers don't hang around forever.
</p>
<h3>Creating an Octopus service account and API key</h3>
<p>
    All workers and tentacles must authenticate themselves with the Octopus server. For shared resources like workers it is best to create a dedicated Octopus user account to grant access to the server.
</p> 
<p>
    Octopus supports the notion of service accounts to represent external machines connecting to the Octopus server.
</p>
<p class="note">
    Because they represent external machines, service accounts can not log into the Octopus web interface.
</p>
<p>
    To create a new service account, click <b>Configuration -> Users</b>, and then click the <b>CREATE SERVICE ACCOUNT</b> button. Enter <b>Test Worker</b> as the username, and <b>Account for test workers hosted in the Kubernetes cluster</b> for the description. Then click the <b>SAVE</b> button.
</p>
<p>
    Once the service account is created, a new section called <b>API Keys</b> is displayed. Expand it and click the <b>NEW API KEY</b> button. Set the purpose of the new key to <b>Test Worker</b> and click the <b>GENERATE NEW</b> button. Make a note of the API key that is displayed, as this value can not be retrieved again once the dialog is closed.
</p>
<p>
    The service account has been assigned to the <b>Everyone</b> team by default. This team doesn't grant the required permissions to register a new tentacle, so we'll need to create a new team.
</p>
<p>
    Click the <b>Teams</b> link, and then click the <b>ADD TEAM</b> button. Give the new team the name of <b>Worker Registration</b> and click the <b>SAVE</b> button.
</p>
<p>
    Under the <b>MEMBERS</b> tab click the <b>ADD MEMBER</b> button. Select the newly created service account and click the <b>ADD</b> button.
</p>
<p>
    Under the <b>USER ROLES</b> tab click the <b>INCLUDE USER ROLE</b> button. Select the <b>Environment Manager</b> role, and click the <b>APPLY</b> button. Then click the <b>SAVE</b> button to save the team.
</p>
<p>
    Our service account is now configured with the permissions it needs to register new workers. The next step is to deploy the workers into the Kubernetes cluster.
</p>
<h3>Deploying workers in Kubernetes</h3>
<p>
    Now that we have a worker pool to host our workers and a machine policy to clean up any workers that have been deleted, we can create the workers in the Kubernetes cluster.
</p>
<p>
    Because tentacles are provided as a Docker image, deploying workers into a Kubernetes cluster is the same process as deploying any other container. This means we will use the <b>Deploy Kubernetes containers</b> step just as we did to deploy the containers in <a href="#chapter1">chapter 1</a>. 
</p>
<p>
    In the <b>Kubernetes DevOps</b> project create a new runbook called <b>Create Kubernetes Workers</b>, and add the <b>Deploy Kubernetes containers</b> step to the process.
</p>
<p>
    Previously, to configure the <b>Deploy Kubernetes containers</b> step we entered details into the individual fields. This is a perfectly valid approach to configuring the step, but there is a faster way to populate the values.
</p>
<p>
    Under the <b>Deployment</b> header you will find a section called <b>Edit YAML</b>. Clicking the <b>EDIT YAML</b> button displays a dialog with the YAML representation of the deployment resource. This YAML can be edited, with the changes populated back into the main form. This allows us to take a YAML document describing a Kubernetes deployment resource and paste it into the step, giving us a populated form with just a few clicks.
</p>
<p class="note">
    The <b>Deploy Kubernetes containers</b> step exposes many fields, but will never expose every possible setting in the underlying Kubernetes resources. If the YAML includes unrecognized fields, they will be silently ignored during the import process.
</p>
<p class="note">
    The YAML presented by the step is an approximation of the YAML that is applied to the cluster. To see the YAML that is applied to the cluster, view the verbose logs generated during a deployment.
</p>
<p>
    The YAML below can be pasted into the <b>Deploy Kubernetes containers</b> step to define a deployment with three workers. Be sure to change the value of the <b>ServerUrl</b> and <b>Space</b> environment variables to match your own instance:
</p>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: octopus-worker
  labels:
    app: tentacle
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  revisionHistoryLimit: 1
  replicas: 3
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: tentacle
        octopusexport: OctopusExport
    spec:
      containers:
        - name: worker
          image: index.docker.io/octopusdeploy/tentacle
          env:
            - name: MachinePolicy
              value: Test Worker
            - name: Space
              value: '#{Octopus.Space.Name}'
            - name: ACCEPT_EULA
              value: 'Y'
            - name: TargetWorkerPool
              value: Testing
            - name: ServerUrl
              value: 'https://myinstance.octopus.app'
            - name: ServerApiKey
              value: '#{ApiKey}'
            - name: ServerPort
              value: '10943'
          resources:
            requests:
              memory: 256Mi
            limits:
              memory: 1Gi
          securityContext:
            privileged: true
</pre>
<p>
    There are two important implications in the way these workers have been configured.
</p>
<p>
    Note that the <b>ServerPort</b> environment variable has been set to <b>10943</b>. This setting defines the port on the Octopus server to connect a polling tentacle to, and by including this environment variable we have also implicitly configured the worker as a polling tentacle.
</p>
<p>
    Tentacles have two modes of operation, polling and listening.
</p>
<p>
    In polling mode, the tentacle reaches out to the Octopus server to receive any new instructions. This means the tentacle instance does not need to expose any ports, as all network traffic is initiated from the tentacle to the server:
</p>
<div><img alt="Octopus environments" src="./images/octopus/polling.png"/></div>
<p>
    In listening mode, the tentacle receives traffic directly from the Octopus server. This means the tentacle must have a fixed hostname or IP address and open port (10933 by default) that the Octopus server can connect to:
</p>
<div><img alt="Octopus environments" src="./images/octopus/listening.png"/></div>
<p>
    Polling tentacles introduce a short delay between when the Octopus server initiates a deployment and the tentacle polls the server for new instructions. Listening tentacles do not have this delay as the server will reach out directly when work needs to be performed.
</p>
<p>
    However, polling tentacles remove the overhead of a static host name or IP address and open ports. This makes them much easier to configure, potentially more secure, and in our case removes the need to configure a service to expose the pods hosting the tentacle containers. For these reasons we have configured our workers as polling tentacles.
</p>
<p>
    The second setting to pay attention to is <b>privileged</b>, which has been set to <b>true</b>. Granting the containers privileged mode allows then to run Docker-in-Docker, which is used to run container images in a worker that is itself a Docker image.
</p>
<p>
    If you recall from <a href="#chapter1">chapter 1</a>, our Kubernetes deployments required access to <b>kubectl</b>. The tentacle Docker image does not have <b>kubectl</b> installed, which means we need some way to expose this tool to our deployments.
</p>
<p>
    We have the option of building our own Docker image based on the tentacle image with <b>kubectl</b> baked in. The code below shows a <b>Dockerfile</b> file to achieve this:
</p>
<pre class="wrapPre">
FROM octopusdeploy/tentacle
RUN curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
RUN sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
</pre>
<p>
    The easiest way to consume a custom image in a Kubernetes cluster is to use a public Docker repository. Docker Hub is one popular option, and offers a free account for hosting public images. My account on Docker Hub is <b>mcasperson</b>. This username will be used in the commands below, but you will need to replace my name with your account name when running the commands locally.
</p>
<p>
    To build the image, run:
</p>
<pre>
docker build . -t mcasperson/k8stentacle
</pre>
<p>
    To log into Docker Hub, run:
</p>
<pre>
docker login
</pre>
<p>
    To publish the image, run:
</p>
<pre>
docker push mcasperson/k8stentacle
</pre>
<p>
    You can now replace the <b>image</b> setting in the YAML file with <b>index.docker.io/mcasperson/k8stentacle</b> to deploy the custom Docker image with <b>kubectl</b> installed. In this case, we no longer need to use Docker-in-Docker, and the <b>privileged</b> flag can be set to <b>false</b> (or not defined at all, in which case it will default to false). To disable Docker-in-Docker on the tentacle we also need to set the <b>DISABLE_DIND</b> environment variable to <b>Y</b>:
</p>
<pre class="wrapPre">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: octopus-worker
  labels:
    app: tentacle
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  revisionHistoryLimit: 1
  replicas: 3
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: tentacle
        octopusexport: OctopusExport
    spec:
      containers:
        - name: worker
          # Make sure to use your own image in production. 
          # Do not rely on external images as they are not maintained.        
          image: index.docker.io/mcasperson/k8stentacle
          env:
            - name: MachinePolicy
              value: Test Worker
            - name: Space
              value: '#{Octopus.Space.Name}'
            - name: ACCEPT_EULA
              value: 'Y'
            - name: TargetWorkerPool
              value: Testing
            - name: ServerUrl
              value: 'https://myinstance.octopus.app'
            - name: ServerApiKey
              value: '#{ApiKey}'
            - name: ServerPort
              value: '10943'
            - name: DISABLE_DIND
              value: 'Y'
        resources:
            requests:
              memory: 256Mi
            limits:
              memory: 1Gi
</pre>
<p class="note">
    Don't use the image <b>mcasperson/k8stentacle</b> in your own cluster, as it is not maintained, and has been shown here for demonstration purposes. Replace this image with one pushed to your own repository.
</p>
<p>
    Baking tools directly into a tentacle image as we just did with <b>kubectl</b> is a perfectly valid solution. Indeed it is the only solution in environments that do not support privileged containers; GKE with autopilot and Azure Kubernetes Service (AKS) with virtual nodes are two examples where privileged containers are not supported.
</p>
<p>
    However, the downside to creating your own tentacle images is that you are responsible for updating the image with new tool versions and redeploying workers. Lifting these requirements from the base operating system (or base Docker image in our case) into a specialized Docker image is one of the main benefits of container images. 
</p>
<p>
    So, looping back to the reason why we set the <b>privileged</b> mode to <b>true</b>; running a container image in a worker that is itself a Docker container requires running Docker-in-Docker, and running Docker-in-Docker requires privileged containers. With privileged worker containers we can continue to use container images as we did when deploying our application in <a href="#chapter1">chapter 1</a>, and take advantage of all their benefits. For this reason the example tests shown below assume Docker-in-Docker is enabled and container images can be used.
</p>
<p>
    The final step is to add the API key generated against the service account earlier as a secret variable called <b>ApiKey</b>.
</p>
<p>
    Go ahead and run the runbook. Once complete you will see three new workers appear in your server assigned to the <b>Testing</b> worker pool. We can now run tests through these workers with access to private containers, such as the backend database.
</p>
<h3>Smoke testing the backend database</h3>
<p>
    Our first test will be to ensure the database is up and responding to requests. To do this we'll log into the database and query the available PostgreSQL schemas. This kind of quick, high level test is referred to as a smoke test. This check is very quick and only asserts that the database is present, has the correct credentials configured, and accepts SQL queries. However, if any of those conditions are not met, no further testing is required as we know something went wrong with our deployment.
</p>
<p class="note">
    The term <i>smoke test</i> has been borrowed from electrical engineering, where the first test of a circuit board was to check if any smoke was coming from it.
</p>
<p>
    Enter the process editor of the <b>Random Quotes - Backend</b> project and add a new <b>Run a Script</b> step. This step will run on a worker from the <b>Testing</b> worker pool, and will use the official PostgreSQL Docker image called <b>postgres</b> as the container image. For the inline source code, add the following Bash script:
</p>
<pre>
export PGPASSWORD=#{DatabasePassword}
psql \
    -U postgres \
    -h postgres.backend-development \
    -c "select schema_name from information_schema.schemata;"
</pre>
<p>
    This script uses the <b>psql</b> command line tool included in the <b>postgres</b> Docker image to log into the backend database and execute a simple SQL query listing the available schemas. If the command's exit code is zero, we can be sure that the command succeeded, and therefore that the database has been deployed and is available.
</p>
<p class="note">
    All CLI commands return an exit code. Typically a value of zero indicates success. You can view the exit code with the command <b>echo $?</b> in Bash, and <b>echo $LASTEXITCODE</b> in PowerShell. A non-zero value for the last command executed in a script step will cause the step to fail.
</p>
<p>
    It can be a little hard to understand the layers that went into running this test script, so let's break it down:
</p>
<ul>
    <li>We have a tentacle running as a worker in a pod hosted on Kubernetes.</li>
    <li>We enable Docker-in-Docker inside the container hosting the worker.</li>
    <li>We build a container from the container image in the Docker-in-Docker environment.</li>
    <li>Calamari then runs inside the nested container in the Docker-in-Docker environment.</li>
</ul>
<p>
    The diagram below shows these nested layers:
</p>
<div><img alt="Octopus environments" src="./images/octopus/docker-in-docker.png"/></div>
<h3>Using third party container images</h3>
<p>
    The database smoke test used a third party Docker image as a container image. As we saw, this was a convenient way to get access to the PostgreSQL command line tools. There are some limits to which Linux Docker images can be used as a container image though.
</p>
<p>
    The Docker image must be based on a Linux distribution using the GNU C library, or <b>glibc</b>. This includes operating systems like Ubuntu, Debian, and Fedora. 
</p>
<p>
    Linux distributions built on <b>muscl</b>, most notably Alpine, do not support calamari, and cannot be used as a container image.
</p>
<p>
    The operating system must also include a number of dependencies required to support .NET Core applications. The documentation at <a href="https://docs.microsoft.com/en-us/dotnet/core/install/linux">https://docs.microsoft.com/en-us/dotnet/core/install/linux</a> lists the dependencies required for a .NET Core application with popular Linux distributions.
</p>
<p class="note">
    If a third party container is missing a library, it is usually the <b>libicu</b> library. The error <b>Couldn't find a valid ICU package installed on the system</b> indicates the ICU library is missing.
</p>
<p>
    If your chosen Docker image does not have these prerequisites, the easiest solution is to create a custom Docker image based on the image you wish to use, install the required libraries, push the image to a repository like Docker Hub, and select your custom image as the container image. We'll see this when building container images for the integration and end-to-end tests below.
</p>
<h3>Smoke testing the frontend API</h3>
<p>
    Our frontend web application exposes a HTTP API that queries the backend database and provides quotes in a JSON response. Also, as we have already seen, the frontend application provides a web page that displays the quotes. Both the API and the web application would be expected to return a HTTP 200 response code after they are deployed. We'll confirm this with another smoke test.
</p>
<p>
    Open the <b>Random Quotes - Frontend</b> project and add a <b>Run a Script</b> step. As before set the step to run on a worker from the <b>Testing</b> worker pool, and this time use the official <b>octopusdeploy/worker-tools:ubuntu.18.04</b> container image.
</p>
<p>
    The inline script Bash code is set to the following:
</p>
<pre class="wrapPre">
# Contact the HTTP API
curl -sSf http://webapp.frontend-development/api/quote > /dev/null

# Fail if curl failed
if [[ $? != 0 ]]; then
	return 1
fi;

# Contact the web frontend
curl -sSf http://webapp.frontend-development > /dev/null
</pre>
<p>
    This script uses <b>curl</b> to make a request to the API and web frontend. The <b>-s</b> flag configures silent mode, the <b>-S</b> flag will display an error if the request fails, and the <b>-f</b> flag prevents generic server side error pages from being displayed.
</p>
<p>
    We need to check if the first request failed by comparing the exit code held in the variable <b>$?</b> to zero. If it is not zero, the script will exit with code 1 and the deployment will fail.
</p>
<p>
    The test then confirms that the URL displaying the web page can also be contacted by <b>curl</b>. If this second <b>curl</b> commands succeeds, the step succeeds. If the command fails, the step will fail as the last exit code will be non-zero.
</p>
<p class="note">
    The specific exit code we pass back to Octopus isn't important for these tests. As long as a non-zero exit code is returned on failure, the Octopus step will fail.
</p>
<p>
    If this smoke tests succeeds we can be sure that the frontend web application has been deployed and is responding to network requests. This test does not verify that the responses are valid though. For that we'll create an integration test.
</p>
<h3>Performing an integration test</h3>
<p>
    Integration tests are used to validate that two or more systems are working together as expected. In our simple application stack we only have two systems: the frontend web application and the backend database. To verify that they are working together, we will use a tool called Postman to write a test script calling the HTTP API to ensure that it is returning a non-empty string for the quote. This ensures both that the frontend HTTP API is accessible, and that it can query the backend database.
</p>
<p>
    Postman is a comprehensive tool in its own right, with far too many features to cover in this book. In this example we'll do the bare minimum to get a test written and exported to run in our cluster.
</p>
<p>
    Download and install Postman from <a href="https://www.postman.com/">https://www.postman.com/</a>. Once installed, create a new collection called <b>RandomQuotes</b>. Inside the collection, add a request that performs a HTTP GET to http://webapp.frontend-development/api/quote.
</p>
<p>
    Under the <b>Tests</b> tab, enter the following script. This test ensures the HTTP response is OK (i.e. a HTTP 200 response code), has a JSON body, and the <b>quote</b> property in the JSON response is not an empty string:
</p>
<pre class="wrapPre">
pm.test("response must be valid and have a body", function () {
    pm.response.to.be.ok;
    pm.response.to.be.withBody;
    pm.response.to.be.json;

    pm.expect(pm.response.json().quote != "").to.be.true;
});
</pre>
<p>
    Click the triple dot menu next to the collection and select the <b>Export</b> option. Save the resulting JSON file to your local PC. The file will look something like this:
</p>
<pre class="wrapPre">
{
    "info": {
        "_postman_id": "2c6a5ce5-e2af-4f23-ab6c-172d0dfa3797",
        "name": "RandomQuotes",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Quotes API",
            "event": [
                {
                    "listen": "test",
                    "script": {
                        "exec": [
                            "pm.test(\"response must be valid and have a body\", function () {\r",
                            "     pm.response.to.be.ok;\r",
                            "     pm.response.to.be.withBody;\r",
                            "     pm.response.to.be.json;\r",
                            "\r",
                            "     pm.expect(pm.response.json().quote != \"\").to.be.true;\r",
                            "});"
                        ],
                        "type": "text/javascript"
                    }
                }
            ],
            "request": {
                "method": "GET",
                "header": [],
                "url": {
                    "raw": "http://webapp.frontend-development/api/quote",
                    "protocol": "http",
                    "host": [
                        "webapp",
                        "frontend-development"
                    ],
                    "path": [
                        "api",
                        "quote"
                    ]
                }
            },
            "response": []
        }
    ]
}
</pre>
<p>
    Add the Postman collection JSON file to a ZIP package called <b>IntegrationTest.1.0.0.0.zip</b>. In Linux and macOS, run the command:
</p>
<pre>
zip \
    IntegrationTest.1.0.0.0.zip \
    RandomQuotes.postman_collection.json
</pre>
<p>In Windows, run the following PowerShell:</p>
<pre>
Compress-Archive `
    -LiteralPath .\RandomQuotes.postman_collection.json `
    -DestinationPath IntegrationTest.1.0.0.0.zip
</pre>
<p>
    The resulting ZIP file then needs to be uploaded to the Octopus built-in feed. In the Octopus interface, click <b>Library -> Packages</b>, and then click the <b>UPLOAD PACKAGE</b> button. Select the ZIP file and click the <b>UPLOAD</b> button. The built-in feed now has a package called <b>IntegrationTest</b> at version <b>1.0.0.0</b>.
</p>
<p>
    We now need a Docker image to use as a container image. The base newman (newman is the Postman command line tool) image is one example of a third party Docker image that doesn't include the required dependencies to run calamari. Specifically we need to install <b>libicu</b> to support the execution of calamari. First create a file called <b>Dockerfile</b> with the following contents:
</p>
<pre>
FROM postman/newman:5.2.0-ubuntu
RUN apt-get update; \
    apt-get install -y libicu60
</pre>
<p>
    Then build the image with the command:
</p>
<pre>
docker build . -t mcasperson/newman
</pre>
<p>
    Finally, publish the image with the command:
</p>
<pre>
docker push mcasperson/newman
</pre>
<p>
    To run the test we need to add a new <b>Run a Script</b> step to the <b>Random Quotes - Frontend</b> deployment process. As before this step will run on a worker from the <b>Testing</b> worker pool. This time though we use <b>mcasperson/newman</b> as the container image.
</p>
<p class="note">
    As with all the custom Docker images shown in this book, replace the username <b>mcasperson</b> with your own Docker Hub username. Also do not use these images in production deployments as they are not maintained.
</p>
<p>
    The inline source code Bash script is set to the following command. This executes <b>newman</b>, which is the command line test runner for Postman:
</p>
<pre class="wrapPre">
newman run IntegrationTest/RandomQuotes.postman_collection.json
</pre>
<p>
    The integration test script is made available as an additional package reference. These additional packages are downloaded, and optionally extracted, into the calamari working directory, and are a convenient way of exposing additional tools and files to a script step.
</p>
<p>
    Expand the <b>Referenced Packages</b> section, click the <b>ADD</b> button, and select the <b>IntegrationTest</b> package from the built-in feed. By default the package will have the name <b>IntegrationTest</b> and is set to be extracted. These defaults are fine, so click the <b>OK</b> button.
</p>
<p>
    The pattern of a script step running in a container image with package references is incredibly powerful. It means a static worker can execute custom tools from a shared container image that reference step specific files downloaded and extracted at runtime. Without this combination of features you would be forced to either build Docker images that embedded files like the integration test, or somehow expose these files to the Kubernetes cluster.
</p>
<p>
    The diagram below shows how the additional packages are exposed in a worker running a container image:
</p>
<div><img alt="Octopus environments" src="./images/octopus/additional-packages.png"/></div>
<p>
    Our integration test is ready to run. Save the changes to the deployment process and deploy the <b>Random Quotes - Frontend</b> project to the <b>Development</b> environment. Once the deployment and smoke tests have passed, newman will run and query the API. The API in turn will contact the database to retrieve a quote. The tests define in the Postman collection then verify the returned data.
</p>
<p>
    For this integration test to pass, both the frontend and backend must be working as expected. So if the test passes, we have a good degree of confidence that our application stack was deployed as expected.
</p>
<p>
    Even if the API is functioning correctly, there is still the possibility that the web application end users interact with was not deployed correctly. To verify the web interface we'll run an end-to-end test.
</p>
<h3>Performing an end to end test</h3>
<p>
    End-to-end tests typically represent the final level of testing. They simulate interacting with the deployed application as an end user would. If an end-to-end test can complete the tasks end users are expected to complete, we gain a high level of confidence that our application is ready to use.
</p>
<p>
    To implement end-to-end tests we'll make use of a tool called Cypress. Cypress provides the ability to interact with a web page as if a human was clicking on the various buttons and links, or populating forms.
</p>
<p>
    Like Postman, Cypress is a comprehensive tool, and has far too much functionality to cover in this book. We'll only touch on the basics required to run a simple test.
</p>
<p>
    A Cypress project is a specific layout of files and directories. In the root directory of a Cypress project we have a file called <b>package.json</b>. This file is used by the Node Package Manager (npm) to install dependencies required to generate specific Cypress reports. In this example we want to use the Mochawesome reporter to generate HTML reports:
</p>
<pre>
{
    "name": "cypress-test",
    "version": "0.0.1",
    "description": "A simple cypress test",
    "dependencies": {
        "mochawesome-merge": "^4.2.0",
        "mochawesome": "^6.2.1",
        "mocha": "^8.2.1"
    }
}
</pre>
<p>
    To download the dependencies listed in the <b>packages.json</b> file run the following command. A directory called <b>node_modules</b> will be created containing all the reporting dependencies:
</p>
<pre>
npm install
</pre>
<p>
    The next file, also in the project root directory, is called <b>cypress.json</b>. This file defines how our test will run and what reports to generate. In this example we specify that we are generating HTML reports, and that our test should open the web application at http://webapp.frontend-development:
</p>
<pre class="wrapPre">
{
    "baseUrl": "http://webapp.frontend-development",
    "reporter": "mochawesome",
    "reporterOptions": {
        "charts": true,
        "overwrite": false,
        "html": true,
        "json": false,
        "reportDir": "."
    }
}
</pre>
<p>
    In the <b>cypress/integration</b> directory we have a file called <b>sample_spec.js</b>. This holds the details of our particular test, which is to click the <b>Refresh</b> button in our web application and ensure the quote text is not empty:
</p>
<pre>
describe('Random Quotes', () => {
    it('Can refresh', () => {
        cy.visit('/')
        cy.get('#refreshQuote').click()
        cy.wait(1000)
        cy.get('#quoteText')
            .should('not.be.empty')
    })
})
</pre>
<p>
    These three files and the <b>node_modules</b> directory are packaged up into an archive called <b>EndToEndTest.1.0.0.0.zip</b>, and the archive is uploaded to the Octopus built-in feed.
</p>
<p class="note">
    A copy of this sample Cypress project can be downloaded from <a href="https://github.com/OctopusSamples/RandomQuotesCypressTest">https://github.com/OctopusSamples/RandomQuotesCypressTest</a>.
</p>
<p>
    We now need to build the container image required to run the Cypress tests. The official Cypress Docker images lack a library required to run calamari, so we must create our own Docker image that includes the required dependencies. We'll also install a Node.js tool called <b>inline-assets</b>. This tool allows us to merge the various files referenced by a HTML page, such as external Cascading Style Sheet (CSS) and JavaScript files, into a single, self contained HTML file. We'll take advantage of this tool later when we generate a HTML report file from Cypress.
</p>
<pre>
FROM cypress/included:6.6.0
RUN apt-get update; \
    apt-get install -y libicu63
RUN npm install -g inline-assets
</pre>
<p>
    Then build the image with the command:
</p>
<pre>
docker build . -t mcasperson/cypress
</pre>
<p>
    Publish the image with the command:
</p>
<pre>
docker push mcasperson/cypress
</pre>
<p class="note">
    As with all the custom Docker images shown in this book, replace the username <b>mcasperson</b> with your own Docker Hub username. Also do not use these images in production deployments as they are not maintained.
</p>
<p>
    To run the end-to-end tests, add a new <b>Run a Script</b> step to the <b>Deploy Random Quotes - Frontend</b> deployment process. The step will run on a worker from the <b>Testing</b> worker pool, and use <b>mcasperson/cypress</b> for the container image.
</p>
<p>
    The inline source code is set to run the following Bash script:
</p>
<pre>
cd EndToEndTest
cypress run > output.txt
RESULT=$?

if [[ -f mochawesome.html ]]
then
    inline-assets \
        mochawesome.html selfcontained.html
    new_octopusartifact \
        "${PWD}/selfcontained.html" "selfcontained.html"
fi

if [[ -d cypress/screenshots/sample_spec.js ]]
then
    zip -r \
        screenshots.zip \
        cypress/screenshots/sample_spec.js
    new_octopusartifact \
        "${PWD}/screenshots.zip" "screenshots.zip"
fi

exit ${RESULT}
</pre>
<p>
    This script starts by entering the directory <b>EndToEndTest</b>, which will be created by extracting an additional package referencing the archive we uploaded to the built-in feed earlier. The <b>cypress</b> tool is run, with the output redirected to a file called <b>output.txt</b>. The exit code from the called to <b>cypress</b> is captured in a variable called <b>RESULT</b>.
</p>
<p>
    If the test succeeded and a HTML report file called <b>mochawesome.html</b> was generated, we then make use of the <b>inline-assets</b> tool to take the HTML report file and merge it, and all of its external CSS and JavaScript files, into a self contained HTML file called <b>selfcontained.html</b>. The self contained report file is then attached to the Octopus deployment as an artifact by calling <b>new_octopusartifact</b>.
</p>
<p class="note">
    We build the self contained HTML report to overcome a limitation in the way Octopus saves artifacts. Each individual artifact is saved at a URL like http://octopusserver/api/Spaces-1/artifacts/Artifacts-1/content or http://octopusserver/api/Spaces-1/artifacts/Artifacts-2/content, where each artifact has its own unique path. This means if we attached the HTML file and its external data files as individual artifacts, each artifact would have a different directory, and opening the HTML file would fail as it expected to load its data files relative to its own URL. By building a self contained HTML file, we can upload it as a single artifact, and open it directly from the Octopus web console.
</p>
<p>
    Cypress will automatically generate screenshots to capture the browser's display in the event of a failed test. These screenshots are saved in the directory <b>cypress/screenshots/sample_spec.js</b>. If this directory exists we'll zip up the screenshots and attach the archive as an artifact. This allows us to debug the test in the event of a failure.
</p>
<p>
    We then exit the step using the exit code from the call to <b>cypress</b>. If the test failed, this exit code will be a non-zero value, and the Octopus step will also fail.
</p>
<p>
    Deploy the <b>Deploy Random Quotes - Frontend</b> project again. If the Cypress tests were successful, an artifact called <b>selfcontained.html</b> is be shown under the deployment <b>TASK SUMMARY</b> tab. This artifact can be opened directly from the link to view the results of the test:
</p>
<div><img alt="Octopus environments" src="./images/octopus/cypress-report.png"/></div>
<p>
    By including an end-to-end test in our deployment process we can gain a high degree of confidence that the deployment succeeded. Cypress is now interacting with the deployed application in the same way we expect end users to, and verifying the web application works as expected.
</p>
<h3>Creating Docker worker images that don't require additional privileges</h3>
<p>
    Making use of container images in workers hosted in a Kubernetes cluster requires them to be privileged. This is a requirement to use Docker-in-Docker. But what can you do if privileged containers are not an option for you?
</p>
<p>
    One option that we have already seen is to add the required tools to the base tentacle image. This is fine for simple tools like <b>kubectl</b>. However, adding complex tools to the tentacle image can be a significant undertaking. For example, adding a tool like Cypress to the tentacle image requires reverse engineering the existing Cypress Docker build files and adding those instructions to the base tentacle image. Even if it were possible to do this, it would be hard to recommend anyone devote the time required to build such an image.
</p>
<p>
    An easier solution is to do the reverse by adding the tentacle on top of the existing Docker image provided by tools like Cypress. We've already seen how to build custom images based on existing images to add dependencies like <b>libicu</b>. Fortunately it is relatively easy to take this process one step further and add the tentacle itself.
</p>
<p>
    The <b>Dockerfile</b> below adds a tentacle as a worker on top of the base Cypress image:
</p>
<pre class="wrapPre">
FROM cypress/included:6.6.0
RUN apt-get update; apt-get install -y libicu63
RUN npm install -g inline-assets
# This is the start of the tentacle install
RUN curl -L \
    https://download.octopusdeploy.com/linux-tentacle/tentacle-6.0.490-linux_x64.tar.gz \
    --output tentacle-linux_x64.tar.gz
RUN mkdir /opt/octopus
RUN tar xvzf tentacle-linux_x64.tar.gz -C /opt/octopus
RUN echo '#!/bin/bash' > /opt/configure.sh
RUN echo '/opt/octopus/tentacle/Tentacle create-instance \
    --instance "Worker" \
    --config "/etc/octopus/Worker/tentacle-Worker.config"' \
    >> /opt/configure.sh
RUN echo '/opt/octopus/tentacle/Tentacle new-certificate \
    --instance "Worker" \
    --if-blank' \
    >> /opt/configure.sh
RUN echo '/opt/octopus/tentacle/Tentacle configure \
    --instance "Worker" \
    --app "/home/Octopus/Applications" \
    --noListen "True" --reset-trust' \
    >> /opt/configure.sh
RUN echo '/opt/octopus/tentacle/Tentacle register-worker \
    --instance "Worker" \
    --force --server "${ServerUrl}" \
    --name `hostname` \
    --comms-style "TentacleActive" \
    --server-comms-port "10943" \
    --apiKey "${ServerApiKey}" \
    --space "${Space}" \
    --workerpool "${TargetWorkerPool}"' \
    >> /opt/configure.sh
RUN echo '/opt/octopus/tentacle/Tentacle agent \
    --noninteractive \
    --instance "Worker"' >> /opt/configure.sh
RUN chmod +x /opt/configure.sh
ENTRYPOINT []
CMD ["/opt/configure.sh"]
</pre>
<p>
    The commands in this Dockerfile, up to the point of the <b>curl</b> command, are the same as we defined in the previous section when building a Cypress container image. Refer to the previous section for more details on these commands.
</p>
<p>
    The <b>curl</b> command downloads a copy of the Linux tentacle archive. We then create a directory called <b>/opt/octopus</b> and extract the TAR file into it.
</p>
<p>
    The next series of <b>echo</b> commands build up a script file that configures and run the tentacle.
</p>
<p>
    The easiest way to get these commands is to install a tentacle on a Linux operating system and run through the configuration script at <b>/opt/octopus/tentacle/configure-tentacle.sh</b>. The final step in the configuration script presents the list of commands that will be run to configure the tentacle. An example of that output is shown below:
</p>
<pre class="wrapPre">
tentacle$ sudo /opt/octopus/tentacle/configure-tentacle.sh
Name of Tentacle instance (default Tentacle):Worker
What kind of Tentacle would you like to configure: 1) Listening or 2) Polling (default 1): 2
Where would you like Tentacle to store log files? (/etc/octopus):
Where would you like Tentacle to install applications to? (/home/Octopus/Applications):
Octopus Server URL (eg. https://octopus-server):
Octopus Server URL (eg. https://octopus-server): https://myserver
Select auth method: 1) API-Key or 2) Username and Password (default 1): 1
API-Key: Select type of Tentacle do you want to setup: 1) Deployment Target or 2) Worker (default 1): 2
What Space would you like to register this Tentacle in? (Default):
What name would you like to register this Tentacle with? (DESKTOP-3E4K4R8):
Enter the worker pools for this Tentacle (comma seperated): Testing
The following configuration commands will be run to configure Tentacle:
sudo /opt/octopus/tentacle/Tentacle create-instance --instance "Worker" --config "/etc/octopus/Worker/tentacle-Worker.config"
sudo /opt/octopus/tentacle/Tentacle new-certificate --instance "Worker" --if-blank
sudo /opt/octopus/tentacle/Tentacle configure --instance "Worker" --app "/home/Octopus/Applications" --noListen "True" --reset-trust
sudo /opt/octopus/tentacle/Tentacle register-worker --instance "Worker" --server "https://myserver" --name "DESKTOP-3E4K4R8" --comms-style "TentacleActive" --server-comms-port "10943" --apiKey "API-XXXXXXXXXXXXXXXXXXXXXXXXXX" --space "Default" --workerpool "Testing"
sudo /opt/octopus/tentacle/Tentacle service --install --start --instance "Worker"
Press enter to continue...
</pre>
<p>
    As you can see, the commands the configuration script would have run have been echoed into the <b>/opt/configure.sh</b> file as part of building our Docker image, with values for the server URL, space, API key, worker pool, and name replaced with environment variables.
</p>
<p>
    The only command we didn't run was <b>/opt/octopus/tentacle/Tentacle service --install --start --instance "Worker"</b>, which starts a systemd service. Our Docker image isn't running systemd, so this command is not required. Instead we run <b>/opt/octopus/tentacle/Tentacle agent --noninteractive --instance "Worker"</b> to start the tentacle manually.
</p>
<p class="note">
    The documentation at <a href="https://octopus.com/docs/infrastructure/deployment-targets/linux/tentacle#installing-tentacle">https://octopus.com/docs/infrastructure/deployment-targets/linux/tentacle#installing-tentacle</a> has details on installing a tentacle on Linux and running the configuration script.
</p>
<p class="note">
    systemd is the software  that runs daemons, or services, in most modern Linux distributions.
</p>
<p>
    To allow the bash script we created with the <b>echo</b> statements to be executed, we add the executable flag via <b>chmod</b>.
</p>
<p>
    The base Cypress image had defined an entrypoint which ran the <b>cypress</b> executable. This makes sense for the Cypress image, but is not useful when our image needs to run the tentacle instead of cypress. So we clear the entrypoint by setting it to an empty array.
</p>
<p>
    Finally we set the script <b>/opt/configure.sh</b> to be run when the container is started. This will configure the tentacle and run it.
</p>
<p>
    To deploy the worker, configure a <b>Deploy Kubernetes containers</b> step with the YAML shown below, replacing the <b>ServerUrl</b> and <b>Space</b> environment variables with your own Octopus server and space. Note that this container does not have the <b>privileged</b> option set, and disables Docker-in-Docker by setting the <b>DISABLE_DIND</b> environment variable to <b>Y</b>:
</p>
<p class="note">
    We have used the system variable <b>Octopus.Space.Name</b> in the example below assuming the workers are being deployed to the same space the runbook is run from.
</p>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: octopus-worker-cypress
  labels:
    app: tentacle
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  revisionHistoryLimit: 1
  replicas: 1
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: tentacle
        octopusexport: OctopusExport
    spec:
      containers:
        - name: worker
          image: index.docker.io/mcasperson/cypressstandalone
          env:
            - name: MachinePolicy
              value: Test Worker
            - name: Space
              value: '#{Octopus.Space.Name}'
            - name: ACCEPT_EULA
              value: 'Y'
            - name: TargetWorkerPool
              value: Testing
            - name: ServerUrl
              value: 'https://myserver.octopus.app'
            - name: ServerApiKey
              value: '#{ApiKey}'
            - name: ServerPort
              value: '10943'
            - name: DISABLE_DIND
              value: 'Y'
          resources:
            requests:
              memory: 256Mi
            limits:
              memory: 1Gi
</pre>
<p>
    Any workers created in this method will support a limited use case. In this example, workers created with the Docker image above are only useful for running Cypress tests. This is the downside to building such specialized images.
</p>
<p>
    However, these images no longer require any special permissions, allowing them to be used universally across different Kubernetes clusters.
</p>
<p>
    If you find yourself in a situation where privileged containers are not an option, adding a tentacle to an existing third party Docker image to create a specialized worker image is your best bet for running tests inside a Kubernetes cluster.
</p>
<h3>Modifying tests for other environments</h3>
<p>
    If you had a keen eye you may have noticed that our tests hard code a URL to the web application in the frontend development namespace, or the hostname of a database in the backend development namespace. If we progress the deployment to the test and production environments, our tests would still be run against the resources deployed to the development environment. Obviously this is not going to achieve the outcome we need, so we need a way to make the tests target the environment we are currently deploying to.
</p>
<p>
    Modifying the front end smoke test is easy as this is an inline script. Update the script to reference the environment name as a variable, like so:
</p>
<pre class="wrapPre">
# Contact the HTTP API
curl -sSf \
    http://webapp.frontend-#{Octopus.Environment.Name | ToLower}/api/quote \
    > /dev/null

# Fail if curl failed
if [[ $? != 0 ]]; then
    return 1
fi;

# Contact the web frontend
curl -sSf \
    http://webapp.frontend-#{Octopus.Environment.Name | ToLower} \
    > /dev/null
</pre>
<p>
    The backend smoke test is likewise updated with the following script:
</p>
<pre class="wrapPre">
export PGPASSWORD=#{DatabasePassword}
psql \
    -U postgres \
    -h postgres.backend-#{Octopus.Environment.Name | ToLower} \
    -c "select schema_name from information_schema.schemata;"
</pre>
<p>
    To update the frontend integration test, we'll use a feature in Octopus called <b>Structured Configuration Variables</b>, which injects values into structured data files such as YAML, JSON, XML, or Java Properties files.
</p>
<p>
    Let's start with the Postman collection JSON file. Because this file is a format recognized by the <b>Structured Configuration Variables</b> feature, we can reuse the existing test and modify it at runtime to suit our needs.
</p>
<p>
    Here is the JSON file again:
</p>
<pre class="wrapPre">
{
    "info": {
        "_postman_id": "2c6a5ce5-e2af-4f23-ab6c-172d0dfa3797",
        "name": "RandomQuotes",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Quotes API",
            "event": [
                {
                    "listen": "test",
                    "script": {
                        "exec": [
                            "pm.test(\"response must be valid and have a body\", function () {\r",
                            "     pm.response.to.be.ok;\r",
                            "     pm.response.to.be.withBody;\r",
                            "     pm.response.to.be.json;\r",
                            "\r",
                            "     pm.expect(pm.response.json().quote != \"\").to.be.true;\r",
                            "});"
                        ],
                        "type": "text/javascript"
                    }
                }
            ],
            "request": {
                "method": "GET",
                "header": [],
                "url": {
                    "raw": "http://webapp.frontend-development/api/quote",
                    "protocol": "http",
                    "host": [
                        "webapp",
                        "frontend-development"
                    ],
                    "path": [
                        "api",
                        "quote"
                    ]
                }
            },
            "response": []
        }
    ]
}
</pre>
<p>
    To edit this file to make it useful for our new service URL we need to update the <b>item:0:request:url:raw</b> property from <b>http://webapp.frontend-development/api/quote</b> to <b>http://webapp.{Octopus.Environment.Name | ToLower}-development</b>.
</p>
<p>
    The syntax we used to create the property <b>item:0:request:url:raw</b> is unique to Octopus, so let's explain each element.
</p>
<p>
    The property is broken down by colons into five items: <b>item</b>, <b>0</b>, <b>request</b>, <b>url</b>, and <b>raw</b>.
</p>
<p>
    The first element of <b>item</b> refers to the array of the same name in the JSON file.
</p>
<p>
    The second element of <b>0</b> indicates that we are selecting the first item in the array. 
</p>
<p class="note">
    JSON arrays are identified by square brackets. Arrays use a zero based index, meaning the index of zero references the first item.
</p>
<p>
    We then continue to traverse the JSON tree. The <b>request</b> element selects the object of the same name inside the first element of the <b>item</b> array. We then select the <b>url</b> object, and finally the <b>raw</b> property.
</p>
<p class="note">
    JSON objects are identified by curly brackets.
</p>
<p>
    We also need to update the <b>item:0:request:url:host:1</b> to <b>frontend-#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    To configure Octopus to make these changes for us we need to create two variables. The first will have the name <b>item:0:request:url:raw</b> and the value <b>http://webapp.frontend-#{Octopus.Environment.Name | ToLower}</b>. The second will have the name <b>item:0:request:url:host:1</b> and the value <b>frontend-#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    Then, in the script step that is running the integration test, click the <b>CONFIGURE FEATURES</b> button, select the <b>Structured Configuration Variables</b> option, and save the changes.
</p>
<p>
    This will display a new section in the step called <b>Structured Configuration Variables</b>. We then list the files that we want Octopus to process in the target files text box. We can enter <b>IntegrationTest/*.json</b> to have Octopus attempt to inject any the values of the two variables we created earlier into any JSON file found from the <b>IntegrationTest</b> additional package reference. The wildcard syntax means our file called <b>RandomQuotes.postman_collection.json</b> will be matched and processed.
</p>
<p>
    We can perform a similar manipulation in the script step running the end-to-end test. Edit that step to enabled the <b>Structured Configuration Variables</b> feature, and then specify the file <b>cypress/cypress.json</b> in the target files text box.
</p>
<p>
    Here is the <b>cypress.json</b> file again:
</p>
<pre>
{
    "baseUrl": "http://webapp.frontend-development",
    "reporter": "mochawesome",
    "reporterOptions": {
        "charts": true,
        "overwrite": false,
        "html": true,
        "json": false,
        "reportDir": "."
    }
}
</pre>
<p>
    We want to update the <b>baseUrl</b> property to  <b>http://webapp.frontend-#{Octopus.Environment.Name | ToLower}</b>. So add an Octopus variable called <b>baseUrl</b> with the value of <b>http://webapp.frontend-#{Octopus.Environment.Name | ToLower}</b>.
</p>
<p>
    By using the <b>Structured Configuration Variables</b> options in our integration and end-to-end test scripts, we can take test scripts generated by external tools with values that are not specific to our infrastructure and modify them at deployment time to suit our needs. Although our test scripts were written to expressly target the original web application in the development environment with the hard coded URL http://webapp.frontend-development, a more common use case is to write test scripts that work on a local PC (for example targeting URLs like http://localhost) and then manipulate them as they are used in multiple environments. Doing so frees the test writers from providing multiple test packages to support each environment, or even from knowing the details of the environments that the tests will eventually be run in. We take care of this at deployment time.
</p>
<h2>Conclusion</h2>
<p>
    The idea of testing code before it is considered suitable to release has won wide adoption amongst developers, and for good reason: catching errors early makes them easier to fix and increases the likelihood that solutions provided to the end user work as expected.
</p>
<p>
    The same benefits of testing apply to your deployment process. By running automated tests as a part of each deployment we can catch errors before they reach the production environment, providing a much better experience for end users.
</p>
<p>
    In this chapter we:
</p>
<ul>
    <li>Discussed the practical decisions around where tests are run.</li>
    <li>Deployed workers directly into the Kubernetes cluster to give tests access to private services.</li>
    <li>Discussed how tentacles implement Docker-in-Docker to allow container images to be run in a Kubernetes cluster.</li>
    <li>Created specialized worker images based on third party Docker images that do not require privileged rights.</li>
    <li>Ran smoke tests against the backend database and frontend web application.</li>
    <li>Ran an integration test against the frontend API using Postman and its command line tool Newman.</li>
    <li>Ran an end-to-end test against the frontend web application using Cypress.</li>
    <li>Used structured variable replacement to modify JSON files for each environment.</li>
    <li>Created a number of custom container images to provide the libraries required by calamari.</li>
    <li>Discussed the limitations of Octopus artifacts, and how they can be overcome with tools that create selfcontained HTML files.</li>
</ul>
<p>
    Now that we have a robust method of ensuring our deployments work as expected, we may find ourselves in a position of having to abort or roll back a failed deployment. The techniques for dealing with failed deployments is addressed in the next pillar of seamless deployments.
</p>