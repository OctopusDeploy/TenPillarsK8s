<h1><a id="chapter2">Verifiable deployments</a></h1>
<p>
    The repeatable deployments pillar describes how promoting releases through environments provides an increasing level of confidence in the solution being delivered. We talked about how frequent deployments to the development environment enabled developers to test their changes, while less frequent deployments to the test environment allowed other parties to verify a potential production release. When everyone is happy, the production environment is updated, exposing the changes to end users.
</p>
<p>
    The pillar of verifiable deployments describes the various techniques that can be used to verify a deployment when it reaches a new environment.
</p>
<h2>General testing concepts</h2>
<p>
    Testing is a nebulous term with often ill-defined subcategories. We will not attempt to provide authoritative definitions of testing categories here. Our goal is to offer a very high level description of common testing practices, and highlight those that can be performed during the deployment process.
</p>
<h2>What don't we test during deployments?</h2>
</p>
    Unit tests are considered part of the build pipeline. These tests are tightly coupled to the code being compiled, and they must succeed for the resulting application package to be built.
</p>
<p>
    Integration tests may also be run during the build process to verify that higher level components interact as expected. The components under test may be replaced with a test double to improve reliability, or live instances of the components may be created as part of the test.
</p>
<p>
    Unit and integration tests are run by the CI server, and any package that is made available for deployment is assumed to have passed all its associated unit and integration tests.
</p>
<h2>What can we test during deployment?</h2>
<p>
    Tests that require a live application or application stack to be accessible are ideal candidates to be run as part of a deployment process.
</p>
<p>
Smoke tests are quick tests designed to ensure that applications and services have deployed correctly. Smoke tests implement the minimum interaction required to ensure services respond correctly. Some examples include:
</p>
<ul>
    <li>An HTTP request of a web application or service to check for a successful response.</li>
    <li>A database login to ensure the database is available.</li>
    <li>Checking that a directory has been populated with files.</li>
    <li>Querying the infrastructure layer to ensure the expected resources were created.</li>
</ul>
<p>
    Integration tests can be performed as part of a deployment as well as during the build. Integration tests validate that multiple components are interacting as you expect. Test doubles may be included with the deployment to stand in for components being verified, or the tests may verify two or more live component instances. Examples include:
</p>
<ul>
    <li>Logging into a web application to verify that it can interact with an authentication provider.</li>
    <li>Querying an API for results from a database to ensure that the database is accessible via a service.</li>
</ul>
<p>
    End to end tests provide an automated way of interacting with a system in the same way a user would. These can be long running tests following paths through the application that require most or all components of the application stack to be working correctly. Examples include:
</p>
<ul>
    <li>Automating the interaction with an online store to browse a catalog, view an item, add it to a cart, complete the checkout, and review the account order history.</li>
    <li>Completing a series of API calls to a weather service to find a city's latitude and longitude, getting the current weather for the returned location, and returning the forecast for the rest of the week.</li>
</ul>
<p>
    Chaos testing involves deliberately removing or interfering with the components that make up an application to validate that the system is resilient enough to withstand such failures. Chaos testing may be combined with other tests to verify the stability of a degraded system.
</p>
<p>
    Usability and acceptance testing typically require a human to use the application to verify that it meets their requirements. The requirements can be subjective, for example, determining if the application is visually appealing. Or the testers may not be technical, and so do not have the option of automating tests. The manual and subjective nature of these tests makes them difficult, if not impossible, to automate, meaning a working copy of the application or application stack must be deployed and made accessible to testers.
</p>
<h2>Verifiable {{ site.platform }} deployments</h2>
<p>
    The first practical consideration that we need to address when testing deployments is where the tests are run.
</p>
<p>
    If you recall from <a href="#chapter1">chapter one</a>, the service we created to expose the database was of type <b>ClusterIP</b>, which means that the service only exposes a private IP address that can be accessed from within the Kubernetes cluster.
</p>
<p>
    We could expose the database to public traffic, but this is an unnecessary security risk given the only clients accessing the database (i.e. the frontend web application) are also deployed in the same cluster.
</p>
<p>
    It then follows that any test that needs to access the database will need to be run within the cluster.
</p>
<p>
    Just as workers can be placed on a cloud VM that has access to a private network, workers can be run as containers inside a Kubernetes cluster. While traditional tentacles and workers are installed directly on a host operating system, tentacles are also provided as Docker images called <b>octopusdeploy/tentacle</b> hosted on Docker Hub.
</p>
<p>
    To create workers in a Kubernetes cluster, we'll need to define a new worker pool, and a new machine policy that automatically cleans up any unavailable workers.
</p>
<h3>Preparing the testing workers in Octopus</h3>
<p>
    We start by creating a new worker pool to hold the workers that will be used for testing. Click <b>Infrastructure -> Worker Pools</b> and click the <b>ADD WORKER POOL</b> button. Give the pool the name of <b>Testing</b>, leave it as the default type of <b>Static</b>, and click the <b>SAVE</b> button.
</p>
<p>
    Pods in a Kubernetes cluster, and the containers they create, are typically considered ephemeral. This means we shouldn't rely on any particular container to run for any length of time. So when running workers in a Kubernetes cluster, we need some way to clean up workers that have been deleted or replaced.
</p>
<p>
    This is where machine policies come in. A machine policy defines the behavior of a tentacle or worker, including how frequently the health and availability of the tentacle is checked, and what to do if a tentacle is unavailable. In our case, we want unavailable workers to be removed from Octopus automatically.
</p>
<p>
    To create a new machine policy, click <b>Infrastructure -> Machine Policies</b>, and then click <b>ADD MACHINE POLICY</b>. Give the policy a new of <b>Test Workers</b>.
</p>
<p>
    We want to test the workers relatively frequently, so under the <b>Health Checks</b> section set the <b>Time Between Checks</b> field to 10 minutes.
</p>
<p>
    We then want any unavailable workers to be deleted from within Octopus, so under the <b>Cleaning Up Unavailable Deployment Targets</b> section set the <b>Behavior</b> option to <b>Automatically delete unavailable machines</b>, and set the time to 15 minutes.
</p>
<p>
    Combined these settings ensure that each worker is contacted by Octopus every 10 minutes to ensure they are available. Unavailable worker will not be selected from the worker pool. Every 30 minutes Octopus will delete unavailable worker, effectively cleaning up the pool and ensuring old workers don't hang around forever.
</p>