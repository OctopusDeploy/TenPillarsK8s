<h1><a id="chapter2">Verifiable deployments</a></h1>
<p>
    The repeatable deployments pillar describes how promoting releases through environments provides an increasing level of confidence in the solution being delivered. We talked about how frequent deployments to the development environment enabled developers to test their changes, while less frequent deployments to the test environment allowed other parties to verify a potential production release. When everyone is happy, the production environment is updated, exposing the changes to end users.
</p>
<p>
    The pillar of verifiable deployments describes the various techniques that can be used to verify a deployment when it reaches a new environment.
</p>
<h2>General testing concepts</h2>
<p>
    Testing is a nebulous term with often ill-defined subcategories. We will not attempt to provide authoritative definitions of testing categories here. Our goal is to offer a very high level description of common testing practices, and highlight those that can be performed during the deployment process.
</p>
<h2>What don't we test during deployments?</h2>
</p>
    Unit tests are considered part of the build pipeline. These tests are tightly coupled to the code being compiled, and they must succeed for the resulting application package to be built.
</p>
<p>
    Integration tests may also be run during the build process to verify that higher level components interact as expected. The components under test may be replaced with a test double to improve reliability, or live instances of the components may be created as part of the test.
</p>
<p>
    Unit and integration tests are run by the CI server, and any package that is made available for deployment is assumed to have passed all its associated unit and integration tests.
</p>
<h2>What can we test during deployment?</h2>
<p>
    Tests that require a live application or application stack to be accessible are ideal candidates to be run as part of a deployment process.
</p>
<p>
Smoke tests are quick tests designed to ensure that applications and services have deployed correctly. Smoke tests implement the minimum interaction required to ensure services respond correctly. Some examples include:
</p>
<ul>
    <li>An HTTP request of a web application or service to check for a successful response.</li>
    <li>A database login to ensure the database is available.</li>
    <li>Checking that a directory has been populated with files.</li>
    <li>Querying the infrastructure layer to ensure the expected resources were created.</li>
</ul>
<p>
    Integration tests can be performed as part of a deployment as well as during the build. Integration tests validate that multiple components are interacting as you expect. Test doubles may be included with the deployment to stand in for components being verified, or the tests may verify two or more live component instances. Examples include:
</p>
<ul>
    <li>Logging into a web application to verify that it can interact with an authentication provider.</li>
    <li>Querying an API for results from a database to ensure that the database is accessible via a service.</li>
</ul>
<p>
    End to end tests provide an automated way of interacting with a system in the same way a user would. These can be long running tests following paths through the application that require most or all components of the application stack to be working correctly. Examples include:
</p>
<ul>
    <li>Automating the interaction with an online store to browse a catalog, view an item, add it to a cart, complete the checkout, and review the account order history.</li>
    <li>Completing a series of API calls to a weather service to find a city's latitude and longitude, getting the current weather for the returned location, and returning the forecast for the rest of the week.</li>
</ul>
<p>
    Chaos testing involves deliberately removing or interfering with the components that make up an application to validate that the system is resilient enough to withstand such failures. Chaos testing may be combined with other tests to verify the stability of a degraded system.
</p>
<p>
    Usability and acceptance testing typically require a human to use the application to verify that it meets their requirements. The requirements can be subjective, for example, determining if the application is visually appealing. Or the testers may not be technical, and so do not have the option of automating tests. The manual and subjective nature of these tests makes them difficult, if not impossible, to automate, meaning a working copy of the application or application stack must be deployed and made accessible to testers.
</p>
<h2>Verifiable {{ site.platform }} deployments</h2>
<p>
    The first practical consideration that we need to address when testing deployments is where the tests are run.
</p>
<p>
    If you recall from <a href="#chapter1">chapter one</a>, the service we created to expose the database was of type <b>ClusterIP</b>, which means that the service only exposes a private IP address that can be accessed from within the Kubernetes cluster.
</p>
<p>
    We could expose the database to public traffic, but this is an unnecessary security risk given the only clients accessing the database (i.e. the frontend web application) are also deployed in the same cluster.
</p>
<p>
    It then follows that any test that needs to access the database will need to be run within the cluster.
</p>
<p>
    Just as workers can be placed on a cloud VM that has access to a private network, workers can be run as containers inside a Kubernetes cluster. While traditional tentacles and workers are installed directly on a host operating system, tentacles are also provided as Docker images called <b>octopusdeploy/tentacle</b> hosted on Docker Hub.
</p>
<p>
    To create workers in a Kubernetes cluster, we'll need to define a new worker pool, and a new machine policy that automatically cleans up any unavailable workers.
</p>
<h3>Preparing the testing workers in Octopus</h3>
<p>
    We start by creating a new worker pool to hold the workers that will be used for testing. Click <b>Infrastructure -> Worker Pools</b> and click the <b>ADD WORKER POOL</b> button. Give the pool the name of <b>Testing</b>, leave it as the default type of <b>Static</b>, and click the <b>SAVE</b> button.
</p>
<p>
    Pods in a Kubernetes cluster, and the containers they create, are typically considered ephemeral. This means we shouldn't rely on any particular container to run for any length of time. So when running workers in a Kubernetes cluster, we need some way to clean up workers that have been deleted or replaced.
</p>
<p>
    This is where machine policies come in. A machine policy defines the behavior of a tentacle or worker, including how frequently the health and availability of the tentacle is checked, and what to do if a tentacle is unavailable. In our case, we want unavailable workers to be removed from Octopus automatically.
</p>
<p>
    To create a new machine policy, click <b>Infrastructure -> Machine Policies</b>, and then click <b>ADD MACHINE POLICY</b>. Give the policy a new of <b>Test Workers</b>.
</p>
<p>
    We want to test the workers relatively frequently, so under the <b>Health Checks</b> section set the <b>Time Between Checks</b> field to 10 minutes.
</p>
<p>
    We then want any unavailable workers to be deleted from within Octopus, so under the <b>Cleaning Up Unavailable Deployment Targets</b> section set the <b>Behavior</b> option to <b>Automatically delete unavailable machines</b>, and set the time to 15 minutes.
</p>
<p>
    Combined these settings ensure that each worker is contacted by Octopus every 10 minutes to ensure they are available. Unavailable worker will not be selected from the worker pool. Every 30 minutes Octopus will delete unavailable worker, effectively cleaning up the pool and ensuring old workers don't hang around forever.
</p>
<h3>Creating an Octopus service account and API key</h3>
<p>
    All workers and tentacles must authenticate themselves with the Octopus server. For shared resources like workers it is best to create a dedicated Octopus user account to grant access into the server. Octopus supports the notion of service accounts to represent external machines connecting to the Octopus server.
</p>
<p class="note">
    Because they represent external machines, service accounts can not log into the Octopus web interface.
</p>
<p>
    To create a new service account, click <b>Configuration -> Users</b>, and then click the <b>CREATE SERVICE ACCOUNT</b> button. Enter <b>Test Worker</b> as the username, and <b>Account for test workers hosted in the Kubernetes cluster</b> for the description. Then click the <b>SAVE</b> button.
</p>
<p>
    Once the service account is created, a new section called <b>API Keys</b> is displayed. Expand it and click the <b>NEW API KEY</b> button. Set the purpose of the new key to <b>Test Worker</b> and click the <b>GENERATE NEW</b> button. Make a note of the API key that is displayed, as this value can not be retrieved again once the dialog is closed.
</p>
<p>
    The service account has been assigned to the <b>Everyone</b> team by default. This team doesn't grant the required permissions to register a new tentacle, so we'll need to create a new team.
</p>
<p>
    Click the <b>Teams</b> link, and then click the <b>ADD TEAM</b> button. Give the new team the name of <b>Worker Registration</b> and click the <b>SAVE</b> button.
</p>
<p>
    Under the <b>MEMBERS</b> tab click the <b>ADD MEMBER</b> button. Select the newly created service account and click the <b>ADD</b> button.
</p>
<p>
    Under the <b>USER ROLES</b> tab click the <b>INCLUDE USER ROLE</b> button. Select the <b>Environment Manager</b> role, and click the <b>APPLY</b> button. Then click the <b>SAVE</b> button to save the team.
</p>
<p>
    Our service account is now configured with the access it needs to register new workers. The next step is to deploy the workers into the Kubernetes cluster.
</p>
<h3>Deploying workers in Kubernetes</h3>
<p>
    Now that we have a worker pool to host our workers and a machine policy to clean up any workers that have been deleted, we can create the workers in the Kubernetes cluster.
</p>
<h3>Deploying workers in a Kubernetes cluster</h3>
<p>
    Because tentacles are provided as a Docker image, deploying workers into a Kubernetes cluster is the same process as deploying any other container. This means we will use the <b>Deploy Kubernetes containers</b> step just as we did to deploy the containers in <a href="#chapter1">chapter 1</a>. In the <b>Kubernetes DevOps</b> project create a new runbook called <b>Create Kubernetes Workers</b>, and add the <b>Deploy Kubernetes containers</b> step to the process.
</p>
<p>
    To configure the <b>Deploy Kubernetes containers</b> step previously we entered details into the individual fields. This is a perfectly valid approach to configuring the step, but there is a faster way to populate the values.
</p>
<p>
    Under the <b>Deployment</b> header you will find a section called <b>Edit YAML</b>. Clicking the <b>EDIT YAML</b> button opens a dialog where the YAML representation of the deployment resource is presented. This YAML can be edited, with the changes populated back into the main form. This means we can take a YAML document describing a Kubernetes deployment and paste it into the step, giving us a populated form with just a few clicks.
</p>
<p class="note">
    The <b>Deploy Kubernetes containers</b> step exposes many fields, but will never expose every possible setting in the underlying Kubernetes resources. If the YAML includes unrecognized fields, they will be silently ignored during the import process.
</p>
<p class="note">
    The YAML presented in by the step is an approximation of the YAML that is applied to the cluster. To see the YAML that is applied to the cluster, view the verbose logs generated during a deployment.
</p>
<p>
    The YAML below can be pasted into the <b>Deploy Kubernetes containers</b> step to define a deployment with three workers. Be sure to change the value of the <b>ServerUrl</b> and <b>Space</b> environment variables to match your own instance:
</p>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: octopus-worker
  labels:
    app: tentacle
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  revisionHistoryLimit: 1
  replicas: 3
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: tentacle
        octopusexport: OctopusExport
    spec:
      containers:
        - name: worker
          image: index.docker.io/octopusdeploy/tentacle
          env:
            - name: MachinePolicy
              value: Test Worker
            - name: Space
              value: MySpace
            - name: ACCEPT_EULA
              value: 'Y'
            - name: TargetWorkerPool
              value: Testing
            - name: ServerUrl
              value: 'https://myinstance.octopus.app'
            - name: ServerApiKey
              value: '#{ApiKey}'
            - name: ServerPort
              value: '10943'
          securityContext:
            privileged: true
</pre>
<p>
    There are two important implications in the way these workers have been configured.
</p>
<p>
    Note that the <b>ServerPort</b> environment variable has been set to <b>10943</b>. This setting defines the port on the Octopus server to connect a polling tentacle to, and by including this environment variable we have also implicitly configured the worker as a polling tentacle.
</p>
<p>
    Tentacles have two modes of operation, polling and listening.
</p>
<p>
    In polling mode, the tentacle reaches out to the Octopus server to receive any new instructions. This means the tentacle instance does not need to expose any ports, as all network traffic is initiated from the tentacle to the server:
</p>
<div><img alt="Octopus environments" src="images/octopus/polling.png"/></div>
<p>
    In listening mode, the tentacle receives traffic directly from tye Octopus server. This means the tentacle must have an fixed hostname or IP address and open port (10933 by default) that the Octopus server can connect to:
</p>
<div><img alt="Octopus environments" src="images/octopus/listening.png"/></div>
<p>
    Polling tentacles introduce a short delay between when the Octopus server initiates a deployment and the tentacle polls the server for new instructions. Listening tentacles do not have this delay as the server will reach out directly when work needs to be performed.
</p>
<p>
    However, polling tentacles remove the overhead of a static host name or IP address and open ports. This makes them much easier to configure, potentially more secure, and in our case removes the need to configure a service to expose the pods hosting the tentacle containers. For these reasons we have configured our workers as polling tentacles.
</p>
<p>
    The second setting to pay attention to is <b>privileged</b> set to <b>true</b>. Granting the containers privileged mode allows then to run Docker-in-Docker, which is used to run container images in a worker that is itself a Docker image.
</p>
<p>
    If you recall from <a href="#chapter1">chapter 1</a> our Kubernetes deployments required access to <b>kubectl</b>. The tentacle Docker image does not have <b>kubectl</b> installed, which means we need some way to expose this tool to our deployments.
</p>
<p>
    We have the option of building our own Docker image based on the tentacle image and baking <b>kubectl</b> into the image. The code below shows a <b>Dockerfile</b> file to achieve this:
</p>
<pre>
FROM octopusdeploy/tentacle
RUN curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
RUN sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
</pre>
<p>
    The easiest way to consume a custom image in a Kubernetes cluster is to have a public Docker repository. Docker Hub is one popular option, and offers a free account for hosting public images. My account on Docker Hub is <b>mcasperson</b>. This username will be used in the commands below, but you will need to replace my name with your account name when running the commands locally.
</p>
<p>
    To build the image, run:
</p>
<pre>
    docker build . -t mcasperson/k8stentacle
</pre>
<p>
    To log into Docker Hub, run:
</p>
<pre>
    docker login
</pre>
<p>
    To publish the image, run:
</p>
<pre>
    docker push mcasperson/k8stentacle
</pre>
<p>
    You can now replace the <b>image</b> setting in the YAML file with <b>index.docker.io/mcasperson/k8stentacle</b> to deploy the custom Docker image with <b>kubectl</b> installed. In this case, we no longer need to use Docker-in-Docker, and the <b>privileged</b> flag can be set to <b>false</b> (or not defined at all, in which it will default to false). To disable Docker-in-Docker on the tentacle we also need to set the <b>DISABLE_DIND</b> environment variable to <b>Y</b>:
</p>
<pre>
apiVersion: apps/v1
kind: Deployment
metadata:
  name: octopus-worker
  labels:
    app: tentacle
spec:
  selector:
    matchLabels:
      octopusexport: OctopusExport
  revisionHistoryLimit: 1
  replicas: 3
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: tentacle
        octopusexport: OctopusExport
    spec:
      containers:
        - name: worker
          # Make sure to use your own image in production. Do not rely on external images as they are not maintained.        
          image: index.docker.io/mcasperson/k8stentacle
          env:
            - name: MachinePolicy
              value: Test Worker
            - name: Space
              value: MySpace
            - name: ACCEPT_EULA
              value: 'Y'
            - name: TargetWorkerPool
              value: Testing
            - name: ServerUrl
              value: 'https://myinstance.octopus.app'
            - name: ServerApiKey
              value: '#{ApiKey}'
            - name: ServerPort
              value: '10943'
            - name: DISABLE_DIND
              value: 'Y'
</pre>
<p class="note">
    Don't use the image mcasperson/k8stentacle in your own cluster, as it is not maintained, and has been used only for demonstration purposes. Replace this image with one pushed to your own repository.
</p>
<p>
    Baking tools directly into a tentacle image as we just did with <b>kubectl</b> is a perfectly valid solution. Indeed it is the only solution in environments that do not support privileged containers - GKE with autopilot and Azure Kubernetes Service (AKS) with virtual nodes are two examples where privileged containers are not supported.
</p>
<p>
    However, the downside to creating your own tentacle images is that you are responsible for updating the image with new tool versions and redeploying workers. Lifting these requirements from the base operating system (or base Docker image in our case) into a specialized tools image is one of the main purposes of container images. 
</p>
<p>
    So, looping back to the reason why we set the <b>privileged</b> mode to <b>true</b>; running a container image in a worker that is itself a Docker container requires running Docker-in-Docker, and running Docker-in-Docker requires privileged containers. With privileged worker containers we can continue to use container images as we did when deploying out application in <a href="#chapter1">chapter 1</a>.
</p>
<p>
    The final step is to add the API key generated against the service account earlier as a secret variable called <b>ApiKey</b>.
</p>
<p>
    Go ahead and run the runbook. Once complete you will see three new workers appear in your server assigned to the <b>Testing</b> worker pool. We can now run tests through these works with access to private containers, such as the backend database.
</p>
<h3>Smoke testing the database</h3>
<p>
    Our first test will be to ensure the database is up and responding to requests. To do this we'll log into the database and query the 
</p>